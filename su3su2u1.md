su3su2u1's tumblr is now dead, so I thought it would be a good time to put together a "condensed" version of his blog that compacts most of his posts into one page. 

This page supplements [danluu's su3su2u1 archives](https://danluu.com/su3su2u1/), but you do not need to have read danluu's archives to make sense of this page. The posts have been slightly edited to fix typos etc. Keep in mind these were all originally posted online in the period 2014-2016 (mostly 2015). 

<em>Since there was some controversy over su3su2u1’s identity, I’ll note that I am not su3su2u1 and that hosting this material is neither an endorsement nor a sign of agreement.</em>

Table of contents:
- [The utility of learning science](#the-utility-of-learning-science)
- [The utility of arguing with cranks](#the-utility-of-arguing-with-cranks)
- [Trust the expert consensus](#trust-the-expert-consensus)
- [There is no general "correctness factor" / Expertise is domain specific](#there-is-no-general-correctness-factor--expertise-is-domain-specific)
- [Beware of epiphanies and big ideas](#beware-of-epiphanies-and-big-ideas)
- [Expected value, Risk aversion and Pascal's wager](#expected-value-risk-aversion-and-pascals-wager)
- [If your results are highly counterintuitive, they are almost certainly wrong.](#if-your-results-are-highly-counterintuitive-they-are-almost-certainly-wrong)
- [Productivity advice](#productivity-advice)
- [Bad mathematical pedagogy](#bad-mathematical-pedagogy)
- [Preparation for undergrad](#preparation-for-undergrad)
- [Taking something seriously means criticizing it](#taking-something-seriously-means-criticizing-it)
- [Real world machine learning ethics](#real-world-machine-learning-ethics)
- [FizzBuzz is useless as an interview question](#fizzbuzz-is-useless-as-an-interview-question)
- [In favor of methodological pluralism](#in-favor-of-methodological-pluralism)
- [Work anecdotes](#work-anecdotes)
- [Undergrad research experiences](#undergrad-research-experiences)
- [It's not worth worrying about "AI risk"](#its-not-worth-worrying-about-ai-risk)
- [Does collaboration "magnify" intelligence?](#does-collaboration-magnify-intelligence)
- [Incentives in scientific research](#incentives-in-scientific-research)
- [Anecdotes about tutoring kids](#anecdotes-about-tutoring-kids)
- [Recommended physics textbook](#recommended-physics-textbook)
- [Traditional vs liberal values](#traditional-vs-liberal-values)
- [The "benefit" of reading fiction](#the-benefit-of-reading-fiction)
- [Szilard Engines, Information and Thermodynamics](#szilard-engines-information-and-thermodynamics)
- [Renormalization group and deep learning](#renormalization-group-and-deep-learning)
- [The stockholder view](#the-stockholder-view)
- [On Bayesianism](#on-bayesianism)
- [Could a super AI figure out relativity?](#could-a-super-ai-figure-out-relativity)
- [IQ stuff](#iq-stuff)
- [Disagreements with Less Wrong](#disagreements-with-less-wrong)
- [Recommended reading instead of HPMOR](#recommended-reading-instead-of-hpmor)
- [On the "sequences"](#on-the-sequences)
- [On transhumanism](#on-transhumanism)
- [The sum of all natural numbers = -1/12](#the-sum-of-all-natural-numbers---112)
- [Anonymous MIRI ask](#anonymous-miri-ask)
- [Map vs territory](#map-vs-territory)
- [Against secretive science](#against-secretive-science)
- [Choice of programming language](#choice-of-programming-language)
- [Statistical methods vs understanding the problem from first principles](#statistical-methods-vs-understanding-the-problem-from-first-principles)
- [Simulation theory of empathy](#simulation-theory-of-empathy)
- [Low model percentages](#low-model-percentages)
- [Cox theorem vs Dempster-Shafer](#cox-theorem-vs-dempster-shafer)
- [Selected HPMOR posts](#selected-hpmor-posts)
    - [Humor](#humor)

# The utility of learning science

aka: "is there a STEM shortage?"

>People I tend to think of as “my kind of people” often seem to bemoan that the “sheeple” out there don’t take an interest in science or engineering.  They often phrase this as “people just don’t seem interested in how things work.”  Sometimes they dress it up in a bit more elitism, and assert that the unwashed masses are simply incapable of understanding.  
>
>But in the last few years I’ve come to realize that knowing as much about how things work as I do is pretty damn irrational.  I service my own car when possible, do my own home repairs, etc.  And in all of this, my time would almost certainly have been better spent if I simply hired someone to do the work.  I know as much physics as pretty much anyone for which I receive 0 economic advantage.  
>
>Bemoaning that people don’t know how their computers or cars work is basically just complaining people don’t share your hobbies. I suspect many more people can learn “how things work” than do, but the way things are organized there is pretty much 0 incentive. 
> 
> >So what do you do with all this physics knowledge?
> 
>Honest answer?  Occasionally talk about it on tumblr.  
> 
>There really isn’t any demand for people who know quantum field theory or what not. Most of us get our phds, do some low-paid science temping for two years or so, and then never do physics again. I taught myself some stats and machine learning and now I do “data science” (its a fancy word for statistics) for anyone who asks. You know what skills I learned in my 10+ years of STEM education that I use in my day to day job?  
>
>1. writing/communication which I mostly picked up in gen ed type classes
>
>2. statistics, which I have basically 0 formal training in
>
>3. programming, which I have basically 0 formal training in 
>
>AND THAT IS IT. You know how often I dip into my physics knowledge at work? Never.  Not even once.  
>
>I was watching the careers of people I respected who graduated before me. The job market meant a lot of scraping and clawing to fight for low paying jobs with no benefits and job security.  I had previously bought into the idea that the most important thing is work you are passionate about, but I started to think that people’s passion was used against them: “oh, you like doing this job? Guess we can pay you next to nothing, treat you poorly, and you’ll still come back for more!” 
>
>I was staring at two career paths when I finished my phd
>
>1. For very low pay, move to another country where my fiance (now wife) would have to be re-licensed in order to work (a process that would take about as long as the expected duration of my position). After 3 years, apply for another one of these positions, forcing my fiance to once again move and relicense (if it was even possible in whatever country I got a job in). Another 3 years, and I have a slim shot at a faculty position somewhere, once again probably moving to a new country.
>
>2. Stay where I was and bartend full time, making 60-70% more than I would make with the postdoc  Use this time to retrain for a new career.  
>
>I chose the latter, but most of the people I know chose the former.  We’ve all ended up in the same place (data sciencey type stuff).  But because I started 6 years earlier than they did, I’m much further along in the career.  
>
>By the time I’d have been begging for the long shot faculty position in track 1, I was being actively recruited to be head of analytics for various insurance in track 2. 
>
>I went to a top program for grad school, one of the best physics schools in the world, my cohort was full of ivy league grads, people who had published in nature and science as undergrads,etc. Fewer than 5% of us were still in science 5 years after we finished grad school. With 1 or 2 exceptions the students in my cohort gained nothing from their phd. It was a 5+ year time-out in their life. They didn’t gain a career, they lost a lot of money in opportunity cost, and they started outside careers that use pretty much none of the skills they spent grad school developing. All of us were told going to a top program meant that as long as we kept our heads down, did our job, and got research out the door, we were on the path to amazing careers. “You might hear bad things about the job market but this is top program, that won’t apply.” There are a fair number of physics professors who have never had a student get an academic job- they don’t really advertise that fact. 
>
>No one (literally no one) will pay you for your knowledge of field theory after graduate school.  So try to pick up some relevant industrial knowledge. Your adviser in all probability DOES NOT KNOW AND DOES NOT CARE what is relevant industrial knowledge. Contact people who have already graduated and gone to Intel, or into consulting, or into data science. Find out what they found useful, find out what would have helped them. During the summers, consider finding internships like engineering phds do. This will build up your list of contacts (and help you find a job later) and these contacts WILL know what skills are useful in industry. 
>
>Do not keep going down this path just because you can’t think of anything else to do.  Do not languish in grad school feeling stuck. There is no shame in quitting- the field of physics DOES NOT VALUE YOU. It uses your passion for interesting work to pay you crappy wages and to keep you moving on the postdoc treadmill.  If your love of the work is enough to sustain you in this harsh reality, more power to you, but there IS interesting work outside of physics. 
>
>Do you want to do grad school because you think research is interesting? Or just to “work for humanity?”  Because every job/task/etc (but especially research) is full of a lot of dull grind.  If you don’t think the research is interesting then you’re stuck with nothing but dull grind, for low pay and bad job prospects.  
>
> > Is there any particular skill you feel that all scientists should have?
> 
> Not really skills but traits- curiosity and perseverance in the face of failure and criticism. When your data looks weird somewhere, or something doesn’t make sense with your model you need a willingness to dive in and figure it out despite the fact that it’s going to be dozens (maybe hundreds) of hours of total drudgery. Science is a ton of incredibly tedious tasks, and lots of drudgery and literally the only pay off is satisfying your curiosity.  
> 
> And you’ll fail, a lot. Most experiments don’t work, most ideas that seem brilliant on the surface don’t work. Lots of elegant theories crash and burn when they meet data. So you have to be resilient. And your ideas will be criticized, over and over, at every conference presentation you give, etc. You have to be able to take constructive criticism and disregard the stuff you don’t find constructive.  
>
>I was flipping channels and somehow settled on an episode of that old TV show with Steve Urkel. In the episode, the cool kid Eddie gets hustled at billiards, and Urkel comes in and saves the day because his knowledge of trigonometry and geometry makes him a master at the table.  
>
>I think perhaps this is a common dream of the science fetishist- if only I knew ALL OF THE SCIENCE I would be unstoppable at everything.  Hariezer Yudotter is a sort of wish fulfillment character of that dream.  Hariezer isn’t motivated by curiosity at all really, he wants to grow his super-powers by learning more science.  Its why we can go 10 fucking chapters without Yudotter really exploring much in the way of the science of magic (so far I count one lazy paragraph exploring what his pouch can do, in 10 chapters).  Its why he constantly describes his project as “taking over the world.”  And its frustrating, because this obviously isn’t a flaw to be overcome its part of Yudotter’s “awesomeness." 
>
>I have a phd in a science, and it has granted me these real world super-powers-
>
>1. I fix my own plumbing, do my own home repairs,etc. 
>
>2. I made a robot of legos and a raspberry pi that plays connect 4 incredibly well (my robot sidekick, I guess) 
>
>3. Via techniques I learned in the sorts of books that in fictional world Hariezer uses to be a master manipulator, I can optimize ads on webpages such that up to 3% of people will click on them (that is, seriously, the power of influence in reality. Not Hannibal Lector but instead advertisers trying to squeeze an extra tenth of a percent on conversions), for which companies sometimes pay me
>
>4. If you give me a lot of data, I can make a computer find patterns in it, for which companies sometimes pay me. 
>
>Thats basically it.  Back when I worked in science, I spent nearly a decade of my life calculating various background processes related to finding a Higgs boson, and I helped design some software theorists now use to calculate new processes quickly.  These are the sorts of projects scientists work on, and most days its hard work and total drudgery, and there is no obvious ‘instrumental utility’- BUT I REALLY WANTED TO KNOW IF THERE WAS A HIGGS FIELD.  
>
>And thats why I think the Yudotter character doesn’t feel like a scientist- he wants to be stronger, more powerful, take over the world, but he doesn’t seem to care what the answers are.  Its all well and good to be driven, but most importantly, you have to be curious.   
>
>The majority of physics phds work in jobs like mine (maybe not in insurance/data science, but in financial services, software development,etc).  It’s a waste of human capital.  
>
>Some STEM phds have industrial demand, but a lot, probably most, do not. People postdoc for a decade (making less than a high end bartender) because it’s the only available career in science for them.  
>
>I’ve had the same job title with the same responsibilities at two different companies and had vastly different experiences. As it stands right now, companies give me some of their hardest problems, a pile of their data, and let me do whatever I want to solve them. Why would I want to retire? There is no way I’d have access to the variety of data and interesting problems that working affords me. 
>
>You’ll probably find as you get older you have lots of goals, desires, etc. Life is a series of trade-offs. For instance, I really really wanted to know what anti-matter was, and as a result I was in school for a decade from the start of college until the end of graduate school. I was applying for my first post-college non-academic jobs 10 years after I started college. I wanted to research and teach physics for a living, but that conflicted with lots of other goals (start a family, live in a nice city near the beach so I can surf whenever, consume top shelf whiskey as if its water, etc). If your sole desire in life is to retire by 30, you might be able to do it, but you’ll probably have to give up a lot of other things.  
>
>I think many, if not most people feel as if they are bad at grad school, but by traditional metrics (publications, citations, etc) I was pretty good at grad school and couldn’t find a job in the field at all, some of my cohort were (by traditional metrics) pretty bad at grad school but hit a hot field and are now tenured professors at prominent institutions. If you just get through grad school, you are good enough for the job lottery, and it really is sort of a lottery. There is some freedom in knowing that most of what will affect your career is outside your control (its mostly funding climate).  

# The utility of arguing with cranks

>When I was a young graduate student, late at night, I’d sometimes log on to antirelativity.com or relativitychallenge.com or a webboard for discussing ‘the final theory” and I’d patiently try to explain where they were going wrong. And after email exchanges where I had typed up 30 or 40 pages of physical arguments/basic physics the exchange would derail [the other party gets aggressive or abusive in their language, or they start repeating arguments we had previously worked through, and they had previously given up, thus dooming us into an endless loop unless one of us bows out (which is usually me, because the physics crackpots who wander the internet have a limitless ability to have the same conversations over and over)]. And now more than a decade later, these guys are still on the same sites, making the same arguments, and claiming that they are being arrogantly dismissed out of hand.  
>
>But now I’m older, wiser, and I don’t spend my time arguing with them. There is an opportunity cost to these sorts of things. If I’m consistently puzzling through ideas that I think are wrong, I’m not spending times on ideas that seem promising. 
>
>Unfortunately, life is too short to waste lots of time reading every 9/11 Truther or Obama Birther I come accross. In the case of Chomsky, red flags that stood out were (1) his insistence that most US presidents haven’t been legally president, due to [off-the-wall legal theory] (2) being willing to entertain conspiracy theories about drugs being deliberately introduced into “ghettos” to undermine political organizing there (3) bizarre romanticization of the Great Depression. Given what I later learned about Chomsky’s Cambodian genocide apologetics, not wasting any more time on him feels like a good decision.
>
>A family friend’s daughter passed away because HIV denialist literature convinced her not to take medicine to protect her daughter. She then passed away not long after. She wasn’t aggressively converted, she got her diagnosis, paniced, and turned to the internet for advice. There are real consequences to those terrible ideas. 
>
>My relatives on facebook have an uncanny way of tossing out these obviously fringe websites (I’m thinking infowars and the like) as valid sources. Often, following some ridiculous claim they link to, I stumble into an alternative reality of suppositions.  
>
>You can find an entire alternate encompassing the gamut from physics (Tesla created infinite free wireless energy at Wardenclyffe, you can find “physicists” with cargo cult papers explaining how all the current physicists are wrong) to economics (mostly “Austrian” economics mixed in with all government stats are wrong! “Inflation is low? HA! That is what THEY want you to think!”), and everything in between. Really, it’s just an alternative theory of reality.  
>
>When most of us look around our apartment and notice we have to plug in our devices, this is proof that electricity isn’t yet wireless. When they observe this, it’s proof of a government conspiracy to hide wireless power from the masses. When the bill shows up for the power, it’s proof the government is screwing them. And really, every aspect of day to day life enforces the power of the illuminati or the lizard men- everything looks so normal! Look at these sheeple who think that just because they are paying roughly the same amount for goods as last year that inflation is low! THEY are hiding it all! THEY are so powerful! 
>
>I am about to retire a notebook, so I was going through it before filing it away. I found in the margins some things I’d written down when various people said things that I found… lets say off-kilter. 
>
>On medicine: “Obviously I’d rather have a math phd with journal access diagnosing me” (the alternative dismissed out of hand was having a doctor diagnosing said person)
>
>“Doctors are basically a fake profession.”
>
>On statistics: “Frequentism is just number voodoo." 
>
>"There is no real reasoning or derivation behind something like an F-test.  It could fail at any time without warning.”
>
>On litmus tests for “sanity”: “I use a simple test, people who aren’t signed up for cryonics or at least considering signing up for cryonics aren’t sane." 
>
>On nutrition:" Dietary scientists have killed millions and condemned hundreds of millions more to obesity”
>
>“There is a near 0 correlation between calories consumed and weight gained." 
>
>(I also had one person talk to me for roughly a half hour about neuro-linguistic programming, but I didn’t have any paper or pen to record it as it was happening). 
>
>A good friend of mine worked with a VC firm to do due diligence on what turned out to be something of a perpetual motion machine. For obvious reasons, he recommended not investing. Semi-famous VC partner said he’d looked over the proposal and it was revolutionary and they were obviously going to invest, short-sighted due diligence be damned (pretty sure this is how black light power gets money). 
>
> I reached out to an investor in impossible startup I had talked about previously. Had a long phone call today, in which he explained to me he didn’t invest because he thought they’d ever be a viable business. He invested because he thought between their pitch/charisma and the names of the investors backing them they’d be able to get several rounds of funding, and he’d be able to cash out.
>
>There is this website called shadowstats which contains “corrections” to traditional government reported economics. I always thought that it was nothing more than a silly debating trick for internet libertarians (and lately the hip new neoreactionaries). If someone uses data to undercut the standard libertarian line, the internet libertarian can say “the data is a government conspiracy! Shadowstats says I’m right!" 
>
>I honestly thought no one would be silly enough to try to make actual, important decisions based on shadowstats. But I just got through sitting down with an analyst at a fairly small hedgefund that has been making bets for years based on the spread between various shadowstats measures and the official measures.  Needless to say, they are on the verge of complete implosion and are looking to dress up horrible derivatives as potentially lucrative derivatives so they can unload them (good luck with that).  
>
>I once stumbled into a meeting of the Natural Philosophy Alliance and it’s the most cargo cult stuff imaginable. They have their own journals, conferences,etc all of which are filled with things that look like physics if you kind of squint. 
>
>I also point out “successfully attracted big venture capital names” isn’t always a mark of a sound organization. Black Light Power is run by a crackpot who thinks he can make energy by burning water, and has attracted nearly 100 million in funding over the last two decades, with several big names in energy production behind him. Black Light Power also has a large volume of self-published research, and a handful of peer reviewed papers.  
>
>I think a lot of this boils down to there are predators, and they are going to prey on people, or at least try to.
>
>Extreme ideologies can (and often do) make this easier, and make it harder to have the proper “cultural immune system” to recognize these people. 
>
>There is a reason that religious scammers go after the radical evangelicals, to pick a (maybe?) less controversial example.  
>
>There is a reason that alternative medicine scams go after people with extreme beliefs about big-pharma and the like. 
>
>There is a reason that cash for gold and investment scams go after certain types of extreme libertarians (take a look at alt-right newsletters sometime). 
>
>We don’t actually have a lot of mechanisms for dealing with scams. See, Amway, Herbalife, Tony Robbins, James Arthur Ray. Hell, James Arthur Ray killed someone, served his time, came right back out scamming.   Multi-level marketing scams are huge business- the closest they get to being shut down is when hedge funders go short on them. 

# Trust the expert consensus 

>I’m of the opinion that you don’t need a strong reason to agree with expert consensus, but you do need a strong reason to disagree with expert consensus.  
>
>The guy who says “I don’t really know how a particle accelerator works, but I trust they found the Higgs boson” is being infinitely more sensible than the guy who says “nah, trust me, I read the blogs, there is no Higgs.” 
>
>I feel like many instances of “insanity” (treating health problems with homeopathy, trying to beat the market THIS time, being concerned with terrorism), and maybe even religion are primarily people being “insane” outside the areas of their actual expertise. People are remarkably good at their actual jobs, and can be remarkably cagey at finding workarounds when they need to,etc.  
>
>I’d go a step further and say a lot of the specific failures are people not trusting experts. If you trust the experts, you’ll put your money in a diversified fund. If you trust the experts, you sure as hell won’t treat your health problems with homeopathy,etc. 
>
>The problem I see is that the sequences seem determined to paint everyone as irrational even within their domain of expertise. So you can out-expert the experts by knowing this one weird Bayesian trick. Compare “I can beat the market THIS time” to “I can beat the market with my Bayesian analysis because most of the participants are irrational.” Or “I’m trying to fix my medical problems with homeopathy” to “I can use this alkali water to drastically prolong my life because medicine is a fake science filled with non-Bayesians.”  They are the same mistakes from the other side, so to speak.
>
>My prior is strongly that most of the world works, with tiny cracks here and there because its impossible to optimize on all axes. 
>
>The “civilizational incompetence” prior taken as a given in a lot of the posts by LessWrong is that THE WORLD IS INSANE!!.   
>
>By my prior, if you disagree with most of the world, you are almost certainly wrong.  In theirs, it's a virtue.  
>
>This can also explain why its easy for them to come to really strong opinions on technical matters they have no background in.  
>
>I’m of the view that “the expert consensus is right” should have a strong prior. So if you are low-information, it makes sense to side with consensus (we all do it all the time on most things). If you are siding against the expert consensus, you need more information than someone siding with the expert consensus. 
>
>Most debates on AGW are low information people arguing with low information people (I recently witnessed a “debate” where both sides were unaware that temperature goes like log of `CO_2` concentration).   
>
>I would expect a person to be at least moderately competent in the field they are making expert opinion on. i.e. if someone wants to claim that many worlds is obviously right, I would expect they’d be able to do basic calculations in the most popular quantum mechanics interpretations. 
>
>If I have a strong prior that experts are correct (say 99% of the time), then to decide that the experts are wrong, I need a lot of information (this is why I’m more comfortable disagreeing with people about physics or statistics than I am about baseball or Peruvian culture).  
>
>If someone tells me that many worlds is obviously correct, but they can’t describe consistent histories or do a calculation in that framework, or they can’t describe the preferred basis problem then the issue is obviously that their prior for the probability that experts are correct is an order of magnitude too weak.  
>
>If someone tells me that nanotech can obviously work but they can’t discuss the basic chemistry leading to the sticky fingers problem, etc.   
>
>The core of Less Wrong is supposed to be a sort of pop-Bayes interpretation. And any good pop-Bayesian should tell you that its foolish to hold strong opinions about subjects you have basically no technical grounding in. Can’t do quantum mechanics? You probably should have at most a weak preference for an interpretation of quantum. Don’t know any computation complexity/information theory stuff? Maybe you shouldn’t have a strong opinion about solomonoff induction as a basis for science or AI. Don’t know any physical chemistry? Maybe keep your opinions about nanotech rather weak. 
>
>Thats the paradox of Less Wrong- after a core component about the importance of Bayesian updates,etc, Yudkowsky spends lots of other sequences trying to convince you to take strong opinions on technical subjects given nothing but a few dozen pages of his word. 

# There is no general "correctness factor" / Expertise is domain specific

Related: [ultracrepidarianism](https://rationalwiki.org/wiki/Ultracrepidarianism)

> >Whether there is a general “correctness factor” based on rationality or something and separate from domain-specific knowledge is…something I wonder about a lot. 
>
>Empirically there does not seem to be a general correctness factor. Lots of people are brilliant in a handful of areas and downright silly in others. I know far too many people who are brilliant in one area and batshit in another (as an extreme example, I once had a conversation with Brian Josephson. Sharp, capable physicist who devoted his post nobel career to paranormal research).  
>
>One of the sharpest mathematicians I’ve ever met is a creationist. I worked with a really good data scientist who made a “thinking cap” that is a helmet with built in coils so he can generate different magnetic fields across it, he claims by adjusting the magnetic field around his brain while he meditates he has raised his IQ 15.  
>
>Unfortunately, all I have are anecdotes, but here is another one, I recently did market analysis for a guy who has successfully run a string of businesses (gym franchises, fast food franchises, etc) and who been making consistently above market returns with his invested assets. He is very successfully contrarian across many business domains, and it’s earning him good money. He also believes that Obama was born in Kenya, and that Russian and Chinese communists are paying Mexican and Central Americans to have anchor babies in the US as part of a conspiracy to slowly turn us into socialists. 
>
>Because disagreeing successfully requires domain specific knowledge, it does not generalize or cluster. The “contrarian cluster” is a bad heuristic that will lead you astray.  
>
>If a giant meta study came out tomorrow that proved Gary Taubes was dead right about all of nutrition, it would have no bearing on my estimates of Taubes being correct about any other position.  
>
>Similarly, the extent to which someone agrees with my positions on particle physics and what they might find at the LHC should have no effect on whether or not they agree with me about bad statistical methodology in psychology. It’s two different areas.
>
>It seems foolish to define “agreeing with me on contrarian issues” as “likely to be correct”- it’s just inviting group think. Libertariains might as well define libertarianism as the obviously correct contrarian cluster, but then so could communists, raelians, wiccans, and people who like mayonnaise on pizza. 

# Beware of epiphanies and big ideas

>There were a lot of moments in grad school where I thought "wait...this approach will certainly work..." (like a lightbulb going off) and then like 3 months of working it out I hit a wall and then a collaborator would show that my approach mapped to some approach we knew couldn’t work or something like that. 
>
>Let’s talk about the Fourier transform and entropy, where I know some history (because physics).  This was definitely not an attempt to sit down and create a big idea.  
>
>Daniel Bernouilli, Clairaut, Lagrange, and Euler had used versions of discrete fourier transforms to solve vibrating string, orbit, and the heat equation as early as the 1740s. Fun historical note, Gauss was actually using the fast fourier transform algorithm in the early 1800s for interpolating along orbits.  Lagrange was probably the first to use one of these early trig series for what we’d call analysis now, studying the roots of cubics.  
>
>But it was really Fourier’s 1809 paper on the heat equation that set out the idea that that it was a general technique and produced a general solution to the heat equation. But Fourier’s work would be seen as “physics math” at best today- in the early 1800s there weren’t yet good notions of function or integral.
>
>It was really first Dirichlet and then Riemann that put the fourier transform on firm grounds, this by around 1850.  
>
>So what we have is a bunch of big names, mostly creating the idea independently to solve little problems here and there (Gauss, Euler, Lagrange,etc, all doing technical work), all through the period between 1740-1800s.  Then you have Fourier recognizing that it was general, but not because he wanted to create a big general problem solving tool- rather he wanted to solve the heat equation for an arbitrary source (doing technical work).  
>
>Then you have the giants that helped to really nail down the modern notions of function (people like Dirichlet and Riemann) as part of that technical work putting it on rigorous footing.  All together, you have 100 years from used in little technical work to somewhat clean formalization. Only then could it become a workhorse tool in math.  
>
>I can tell a similar story about entropy.  The non-statistical entropy concept was probably first put forward by one of the Carnot’s (Lazare or Sadi) some time around the 1800s, but it was Clausius who gave it the name entropy, and Clausius’s work in kinetic theory/diffusion (build off earlier work in kinetic theory) was dancing around the ideas that would later become the statistical concept of entropy, work he did in the early 1850s. 
>
>In the early 1860s, Maxwell developed his probabilistic distribution for gas velocities (now often called Maxwell/Botlzmann distribution), Botlzmann would generalize it by the end of the 1860s/early 1870s, and developed his H theorem/the statistical grounding of entropy.  
>
>And then it took Gibbs running with it in the 1870s (along with Boltzmann and Maxwell who continue to work) to get to the more modern canonical ensemble type ideas.  
>
>So again we see many great names (Carnot and Carnot,Clausius, Boltzmann, Maxwell, Gibbs,,etc) groping toward a technical problem (understanding gasses and heat).  It took almost 50 years for entropy to go from “rough idea” as put forward by Carnot to “general concept with a name.”  Ten more years before Maxwell started to grasp the probabilistic nature of what he was looking at, and another 20+ to get to the canonical ensemble.  
>
>And it wasn’t probably until the 1940s that Shannon applied it to cryptography work during the war (which would eventually become his information theory).  
>
>I’m not saying these aren’t big ideas,or that there aren’t big ideas.  Obviously there are.  I’m saying big ideas aren’t bolts of epiphany, there is a lot of slow grinding work, building on other work, aimed at solving specific technical problems.  This stuff then generalizes and turns out to be really big interesting ideas, but that is not at all apparent to the earliest people creating the ideas, and it takes a whole generation to get it to the “big idea finish line.”
>
>Fourier didn’t sit down and say “I want to figure out a general analysis tool…” think really hard and plop it down.  He wanted to solve the heat equation and he took ideas from Lagrange and others (who had taken ideas from others,etc) and realized a technique that lots of people had been using was not only perfect, but more general than they realized.  Even then, it wasn’t a formal or workhorse sort of result until Dirichlet and Riemann got ahold of it. 
>
>Boltzmann didn’t originally set out to plop down entropy fully formed. He was working to generalize Maxwell’s result on kinetic theory and saw more directions to go with it.  That lead to his H-theorem, which created a literature between Maxwell, Gibbs and Boltzmann,etc.  
>
>The single most important lesson in my working life is that epiphanies, “ah-ha” moments, “big ideas” are mostly a bullshit after-the-fact mythologizing.  My best work, my most interesting work, etc isn’t the result of a spot of inspiration or a giant big idea, but instead grinding it out, doing the job, removing the parts that don’t work, grinding some more,etc.  Don’t wait for lightning to strike, just work on what you can and move the ball forward a bit. 
>
>Even when I have a big idea, it’s rare to actually recognize it as a big idea at the time.  One work project started because I was talking to a coworker and we were bitching about data quality (as all data scientists do, pretty much daily) and I realized “hey, maybe we can `<do this thing>` to clean up some of the data,” but this was literally one of hundreds of ideas people in the team have had to wrestle with some of our problems.  And we had to grind it out to make it happen- changing the way some systems did data intake and re-purposing a ton of old code to make the changes we want.  And it worked! And not only did it work, it worked surprisingly well.  But there were literally a hundred ideas we had around this problem, and we tried to implement about a dozen before we arrived at our current system.  
>
>Everyone remembers the conversation `<coworker>` and I had where I came up with this, and it’s become a bit of a myth, but the reality is that we’re glossing over 1. a ton of detail work to get there 2. the fact that at the time this was one idea among many. It’s the fact that it worked that made it after the fact important. 
>
>This was also true in my physics career, but let’s take a big idea guy like Einstein as the example.  In 1907, Einstein described his famous elevator thought experiment and published some preliminary ideas about general relativity.  It took 8 more years of grinding (during which Einstein was mostly publishing on other topics), many papers and ideas, collaborations with Grossman, Fokker and others, as well as correspondence with Hilbert,etc to turn that seed of an idea into the theory of General Relativity (which was published in 1915).  
>
>This crystallized for me when an intern asked me the other day “do you have more breakthroughs in data science than you did in physics?” and I thought “man, this big-idea/breakthrough model of science is a story for the movies and for children…” 
>
>One thing that I think comes of studying something a scientific discipline deeply is that it innoculates you against a lot of the effects people seem to get out of “insight porn.”  
>
>All during my study of physics, I had many “ah ha!” moments, little epiphanies, bursts of insight, but invariably these turned out to be wrong.  When confronted with data, my model was too simple, or I was grouping together mathematical concepts that seemed similar but turned out to be unrelated in little ways.  The idea that these “epiphanies” were aiding my understanding was total illusion, dashed against the shores of the excercises in the book.  
>
>This also shows up in data science- a lot of times the models produced are simply too complicated for a good explanation of the data.  It’s not uncommon to notice some pattern in the model and to create a story around it “oh, what is happening is that X is really driving Y because detailed story.”  The pulled-out-of-my-ass story feels more right, more concrete than the model.  Only later I might find out that I made a mistake in the code, and X and Y are totally uncorrelated.  Of course, then I can create a nice detailed story about why they are obviously uncorrelated.  
>
>And I think a lot of what ties together the appeal of certain reactionary writers, the LW sphere and other sources seems to be these sort of land-mine epiphanies/insights buried in the writing.  They feel good and seem so right until you sit down to do the problems in the textbook.  
>
>“Oh! Bayesianism is the one-true-inference-method! It’s so obvious. ” until you try to do something with it and have the anti-epiphany of “huh… well, what should I use for a prior here?  Huh, here is a good non-Bayes way to do this and I don’t see any Bayes way at all…Maybe this is more complicated than I thought.”  
>
>“Many worlds quantum is obviously correct!  How can those physicists not see this!” never meets “well, the basis problem is really weird… I don’t really like this Deutsch/Wallace decisions stuff… huh… I’m not sure what to do with this…?”  
>
>I don’t want to get to deep into it, but the “anti-reactionary” faq on SSC looks to be full of similar places where the epiphany driven models of reactionaries don’t seem to hold up to data.  
>
>Beware “insight porn,” you could easily be filling your head with over generalized, bad models.  But the just-so stories feel so true… 
>
>I try not read many analyses of class. I find they generally have some insight, but also large doses of generalization-from-stereotype. So you end up with a description that feels real but I’m not sure how much reality it can contain.    
>
>My prototypical example is that David Brooks piece where he traveled to red America/Blue America and talks about the cultural/status differences in the different areas. Problem is, nearly all of his generalizations are actually false.  
>
>Also, from my own life, at an extended family gathering (thanksgiving at my grandmother’s house,say) we’ll have a corporate lawyer,a doctor,a retired engineer,a single mother living on disability for migraines and back pain she doesn’t have, a construction worker, a long haul trucker, and a “reformed” small time con artist who now gives real estate seminars.  One family runs the gamut from various descriptions of low class to various descriptions of upper class with a lot of in between.  
>
> >Do you like SSC?
>
>I generally don’t enjoy what I think of as excessive generalization- taking a few disparate examples and blowing things out to a general principle (or taking things to a “meta” level,etc.). I’m skeptical such generalizations are useful. Rationalists seem to love that sort of stuff, but it’s not my cup of tea.  
>
>It’s always important to turn on your critical thinking when an idea seems appealing- the unappealing ideas will take care of themselves. 
>
> >What do you think of Robin Hanson?
>
>I’ve only read a little of his stuff.  He often seems to postulate “big idea” type theories and then shoe-horns lots of observations into them, which (if you’ve seen my other posts) you might know is opposed to the way I like to build knowledge.  Little models first, then big models that tie them together (if the big models turn out to be warranted).  “Fox not hedgehog”  Anyway, because of this mismatch I usually don’t read him (probably also part of why I don’t like Less Wrong). He also occasionally promotes stuff I think is pretty crackpot. I was going to go looking for an example, and literally [the most recent post](http://web.archive.org/web/20150824140448/http://www.overcomingbias.com/2015/03/life-before-earth.html) is promoting a two year old bit of near-pseudoscience.  
>
>Most ideas that are at first glance awesome really don’t work when you try to generalize, apply them, or confirm them (if you want to make money, bet econ 101 students every year around Thanksgiving if the price of Turkeys are going to go up or down. They’ll tell you a supply and demand story, predict up, and then you’ll win money). Probably the overwhelming majority of such ideas fail.
>
> >Econ 101 supply and demand curves may not be literally accurate. But are they as misleading as a physics prof telling kids, “things like to go in circles”? I don’t think so.
>
>I mean, yeah, they kind of are misleading?  There is nothing apriori limiting the shape of a demand curve- it can take almost any shape at all.  To apply any model in quantitative way, you have to actually go out and try to estimate the demand curve. 
>
>In grad school I used to win bets every year about whether the price of turkeys would go up or down around Thanksgiving (they go down, every year). They had economic theory, I had the advantage of having checked the price of turkeys several months in a row.  
>
>I’ll add that I don’t know anyone who was trained in physics who didn’t become deeply skeptical of the type of modeling done in econ 101 type classes.  
>
>I think if you have a physicists idea of taking data, refining models,etc the economics approach of distlling “stylizied facts” that are to be explained by simple models (that explain some “stylized facts” but not others,etc) seems just wrong.  
>
>Also, economics has these weird issues where the people who actually work in a topic area are less likely to believe the econ 101 line in that area. If you want to find an economist who doubts the econ 101/stylized facts about free trade and comparative advantage, you have to look at the economists who study developing economies. Sure enough, you find people like Dani Rodrik.  
>
>I do know a lot about auto industries, and Japan’s protectionist policies were a huge boon to the birth of Japanese auto companies.  
>
>And China seems to be flourishing under a more sophisticated form of mercantilism (currency pegs to maintain trade surplus).  
>
>My reading of the literature is that there isn’t much evidence for robust economic growth from trade liberalization.  
>
>If you keep shoe-horning observations into a simplified theory, you are just falling victim to the illusion of understanding. You aren’t systematically testing, you are just noticing situations you run into when the model applies (which leaves you victim to all sorts of biases).  
>
>When I regularly worked in physics, I was able to generate neat ideas pretty rapidly, but almost none of them worked out (and a handful mapped on to neat ideas people had in the 80s). Now that I work in data science, a similar story.  
>
>So I guess “postulate giant model of the world, never test it, move on” is just not something I’m interested in. It’s pretty unlikely to get you any truth. 
>
>Tetlock has this classification of judgement styles, foxes and hedgehogs (borrowed from Archilochus).  Foxes know many small things, hedgehogs know one big thing that they try and apply.  
>
>I can’t remember if this was part of Tetlock’s description but the variant of those concepts in my head goes a bit further- fox problem solving involves building small, domain-specific models by poking at a problem in lots of little ways until it yields.  Hedgehogs problem solving involves taking your large, overarching model and whittling it down, making approximations,etc until it applies to the problem at hand.  
>
>By natural inclination, I am much more of a fox than a hedgehog  I think most progress is made by building domain specific models and trying to slowly make them more general, tying lots of little things together.  
>
>Evopsych is sort of the ultimate hedgehog field- they start from a core idea that “evolution has shaped our social psychology” which is almost certainly true, and try to find situations to apply that big idea.  I’m by inclination and experience, very skeptical of this approach. 
>
>I also find that when the evopsych people try to go about the hard part of hedgehogging- turning their large universal model into something specific to a domain, and then testing it, they have a tendency to make statistical errors (like the Kanazawa paper I referenced earlier).  This doesn’t mean that evopsych claims are wrong but it does mean that a lot of them are unsupported.  
>
>I’m much more wary of elegant theories in (say) evolutionary psychology than in physics because the realm of physics is actually really, really simple. There probably are elegant explanations of simple phenomena (and they almost involve the phenomena being a spring in some way- that is a post explaining neutrino masses/the last physics nobel in terms of springs).  
>
>In social sciences, I think the way you get elegance usually involves shaving a lot of rough edges off the phenomena you want to explain, and then focusing on some narrow pieces of that phenomena. There is an adage that you should make your model as simple as possible but no simpler, and I think sometimes elegance/”that epiphany moment” is often a sign your model is too simple. 
>
>As an example from my work, an employee of mine loves to make smooth aggregates of our data, fit them to really nice distributions, and then create these really slick models explaining how the distributions behave.  As a result, he always has these nice explanations of what his model is doing with really well defined error bars on all the predictions (there is an X% chance that Y will fill between A and B), and really easy to understand visualizations.  But I recently had to talk to him because none of his models are any good at actually predicting the future.  
>
>I generally iterate between regressions and causal analysis with a few small assumptions, and what I end up with is an incredibly complicated mess but it does predict the future.  
>
>My opinion is that social science is very hard, and its often using methods that simply can’t resolve core disagreements.  
>
>The other issue is that there is a lot of confusion over methodology- which totally makes sense. The majority of social scientists I know hated methods and just wanted a recipe to follow in a statistical program.  This leads to attacks on (and defenses of) methodology that read as confused to me.  
>
>They are doing what they can with limited data, but I think the research is going to be more suggestive than definitive. It is intellectually dishonest to base conclusions on data known to be bad. Controlling for an entire kitchen sink worth of variables will not, in general, fix bad data. I fear, by this standard, whole fields of inquiry are (at their core) intellectually dishonest. 
>
>I face this a lot (in the very different context of business problems). A lot of times, the recorded data just isn’t close enough to what the client actually cares about that doing any inference is very hard (and thus the final results will be really uncertain.)  
>
>But no one ever responds well to “you took the wrong data, and now you’re going to get a crappy estimate no matter how clever I try to be.” There is a lesson here- the most important part of statistics is the part people generally don’t think about, designing the measurement/data collection process well enough to produce useful data. 
>
>Evolutionary psychology is a field that famously has a pretty poor bullshit filter. Satoshi Kanazawa once published a series of articles that beautiful people will have more female children (because beauty is more important for girls) and engineers/mathematicians will have more male children (because only men need the logic-brains). The only thing his papers proved was that he is bad at statistics (in fact, Kanazawa made an entire career out of being bad at statistics, such is the state of evo-psych).  
>
>One of the core criticisms is that for any fact observed in the world, you can tell several different evolutionary stories, and there is no real way to tell which, if any is actually true.  Because of this, when someone gives you an evopsych explanation for something, its often telling you more about what they believe then it is about science or the world (there are exceptions, but they are rare).  
>
>I’m not huge on signalling theories in general, I think they are overfit and lack predictive power.  Everything is signalling, and knowing that everyone is signalling doesn’t help you predict what someone is actually going to do. 
>
>I recently attended a party hosted by the head of hospital medicine at a large university program. The people attending can largely be broken into 3 categories- 1. wealthy, late career doctors who are heads of various departments. Easily making 500k+ a year. 2. Early/mid career doctors. Solid attending salaries (200k-300k or so), probably some student loan debt. 3. Residents with lots of debt and comparatively low salaries.  
>
>My wife pointed out between half and three-fourths of the women in only 1 of these categories had an ostentatious luxury item similar to a watch (no woman we noticed in the other two categories had a similar item).  
>
>Can you use the signalling model to predict which group? I suggest you cannot- the signalling explanation could predict ANY of those groups having the item.  
>
>If it were 
>
>For 1, you could say “the richest people obviously signal their wealth.”  
>
>If it were 2, you could say “the people who haven’t quite made it find it most important to signal their status.” 
>
>If it were 3, you could say “signalling is forcing these poor residents to waste hard earned money in order to fit in aspirationally with a social group.”  
>
>Care to take a guess? What sort of odds would you put on that guess? 
>
>Similarly, my family makes a routine habit of giving money for Christmas- are we countersignaling? What about cultures where money is the norm? I contend cultural markers are tremendously more predictive than signalling explanations. Signalling is just a nice after-the-fact narrative.  
>
>It was the residents, and they were all carrying specific very expensive hand bags.  But, in this case it might well not be signalling- I later found out via my wife facebook stalking- one of the residents got married recently, had a ton of bridesmaids, and gave expensive hand bags as a bridesmaid gift. So maybe it was signalling to the other residents “look, I got to be in the fancy wedding.” (See, anything can be signalling).  
>
>Anyway I want my models to predict something- “racism exists” won’t predict whether rich people or poor people are more racist (it has nothing to say on the matter). It might instead predict the outcome of job interviews between various candidates of different races, but lets not get into a race discussion.  
>
>The signalling models all seem too weak to make any concrete predictions because they are very vague.  Between signalling, counter-signalling, counter-counter-signalling,etc I can craft a signalling narrative for literally any data or behavior you give me. It’s not even wrong, it’s a just-so story.  
>
>I’m deeply suspicious of explanatory narratives in general, because I’ve had too many experiences where I do some analysis for work and I say “oh, that result makes sense because X,Y and Z.” And then a day later I find a bug, and the real result turns out to be just the opposite. But then I say “oh, that result makes sense because of A,B and C.”  
>
>Cost effectiveness is always a hard thing to estimate.  An insurance company, for instance, won’t take a small portion of their claims and just pay checks without running them through their claims handling department.  The potential cost to the company is just too big- its not worth quantifying if its going to cost that much.  
>
>Some companies, in this situation, use woefully underpowered statistical analysis (usually on horrible data sets that contain data almost completely inappropriate to the task at hand), and then throw out some number.  And “everyone knows” its just a ball park, totally wrong ,etc.  Basically worthless, but maybe its the right order of magnitude.   And then they use it anyway.  
>
>Pretty soon some middle level manager will be saying Lets not give claims more money! They only save us X.  And no one remembers that everyone expects claims does so well that its too expensive to estimate how well they do!   Pretty soon they hire someone like myself to come in and revamp their business intelligence process, because they find its not working.  Things have gotten worse since they went to quantitative management!   The analyst discovers its because they codified all these pulled-out-of-a-hat guesses.  In these cases I almost always tell them “guys, you are pretending like you know like 10 things that you don’t actually know.  Just scrap this whole system, and start collecting good data and maybe in a few years you can try again, with good data. ”
>
>And this applies out side of business.  Lots of social science is full of atrocious papers for similar reason (I’m looking at you education).  Instead of treating a bad paper as the best provisional evidence we have. Maybe its better to say Sorry, we just don’t know.  
>
>This post was inspired by facebook rants I saw today about how kids don’t learn anything in public schools.  Near as I can tell Sorry, we just don’t know how much kids are learning in public schools.  We can’t do a good study, because nearly everyone believes randomly assigning kids to not receive an education does too much harm to make the measurement worth doing.  
>
>This isn’t to say all social science is worthless- there are tons of clever papers that find good data to tease out worthy questions.  Unfortunately, there are also tons of papers where they grabbed the only data set they could find and tortured it until it gave an answer.  Unfortunately, torture is as unreliable a way to get answers out of data as it is with people. 

# Expected value, Risk aversion and Pascal's wager

>If someone offers me $1 games where I have a 1/10^6 chance to win 10^7 dollars, I might behave differently if I have $20 (which I’m nearly certain to lose all of it) then if I have 10^17 dollars, in which case I’m nearly certain to win an average of $10 per game.  So we might want to calculate the variance of the sampling distribution and weight that in our decisions.
>
>I’ve noted in the past that this can solve Pascals wager type situations where both the reward and the variance of the reward head to infinity. 
>
>Alright, in a previous post I brought up the St. Petersburg paradox, but I’ll refresh it here: 
>
>Imagine a lottery that works as follows- you flip a coin. If it is heads, you get $2.  If it is tails, the pot doubles and you flip again.  
>
>So if the first flip is heads you get $2.  If you get tails and then heads you get $4.  tails, tails, heads, you get $8.  So on.  The question- how much is a ticket to play this lottery worth?
>
>The “paradox” is that if you actually play the game you are definitely going to win a finite amount of money.  However, if you calculate the expected value you get 
>
>(½) *2 + (¼)*4+(1/8)*8 + … = 1+1+1+1+1+1…. = infinity.  
>
>So if the fair price for the game is infinite but you can only win a finite amount of money, something is wrong!   
>
>How do you resolve this?  What is going on?
>
>Weirdly, the wikipedia article on the St. Petersburg Lottery has a lot of examples of historical solutions, but devotes only a tiny solution to the actual resolution.  I’m pretty sure I first saw this result in Feller’s textbook, and it can be derived rigorously, but I didn’t see it discussed the way I’m going to.  
>
>The problem with the lottery paradox is a really subtle assumption we are making when we do the expected value calculations- we assume that if we want to calculate the time average of our winnings, we can do this by considering a collection of every possible outcomes, side by side.  
>
>This is a powerful assumption, called ergodicity and it holds in a lot of everyday processes.   When we do thermodynamics we absolutely rely on the ergodic assumption- to calculate the fluctuations in physical quantities we don’t calculate the system evolution over time and average.  Instead we calculate the fluctuations over the entire system (in space).  
>
>The problem is that (weirdly!) the St. Petersburg lottery is not ergodic.  What Feller did was to carefully consider the average payout over the number of games played.  
>
>What you find is that the average payout for n games of the lottery goes like `log_2(n)` (the base 2 log of n).  
>
>I don’t want to prove this (see Feller), but writing simulations is pretty easy- here is a list of average return for the number games played in my simulation
>
>Games played -> avg return
>
>10 -> 5
>
>100 -> 9 
>
>1000-> 11
>
>10000 -> 14
>
>100000 -> 28 
>
>1000000 -> 32
>
>Now, some of these have fluctuated up above the expected `log_2(n)`, but the increase with the number of games is pretty unmistakeable (note, I got lucky, I’d have apriori expected one of these to be ordered incorrectly).  
>
>So the paradox is entirely because the average over repeated games is sensible, but not in the limit of infinite games (where it blows up to infinity).  Trying to calculate EV by taking the ensemble average is just nonsense!  
>
>The fair price of the St. Petersburg lottery depends on the number of games you are allowed to play.  (Note: this ties together a lot of the responses you see like “the banker will eventually go broke,etc”  If the banker will go broke, then the maximum number of games becomes naturally limited).  
>
>I wanted to talk about this because we make all these little assumptions all the time and it’s like breathing- we stop thinking about it.  Also, judging by wikipedia this isn’t something widely known! 
>
>ALSO (and this is weird) our intuition mostly gets it right here! I don’t think many people would pay more than a few dollars for the lottery which is right in the somewhere between 10 and 100 game sweet spot.  Shifting to the math (shut up and multiply!) we make an incredibly subtle mistake! 

# If your results are highly counterintuitive, they are almost certainly wrong.  

>Once, when I was a young, naive data scientist I embarked on a project to look at individual claims handlers and how effective they were.  How many claims did they manage to settle below the expected cost?  How many claims were properly reserved?  Basically, how well was risk managed?  
>
>And I discovered something amazing! Several of the most junior people in the department were fantastic, nearly perfect on all metrics. Several of the most senior people had performance all over the map.  They were significantly below average on most metrics! Most of the claims money was spent on these underperformers! Big data had proven that a whole department in a company was nonsense lunacy! 
>
>Not so fast. Anyone with any insurance experience (or half a brain, or less of an arrogant physics-is-the-best mentality) would have realized something right away- the kinds of claims handled by junior people are going to be different. Everything that a manager thought could be handled easily by someone fresh to the business went to the new guys. Simple cases, no headaches, assess the cost, pay the cost, done. 
>
>Cases with lots of complications (maybe uncertain liability, weird accidents, etc) went to the senior people. Of course outcomes looked worse, more variance per claim makes the risk much harder to manage. I was the idiot, and misinterpreting my own results! 
>
>A second example occured with a health insurance company where an employee I supervised thought he’d upended medicine when he discovered a standard-of-care chemo regiment lead to worse outcomes then a much less common/”lighter” alternative. Having learned from my first experience, I dug into the data with him and we found out that the only cases where the less common alternative was used were cases where the cancer had been caught early and surgically removed while it was localized.  
>
>Since this experience, I’ve talked to startups looking to hire me, and startups looking for investment (and sometimes big-data companies looking to be hired by companies I work for), and I see this mistake over and over. “Look at this amazing counterintuitive big data result!”  
>
>The latest was in a trade magazine where some new big data company claimed that a strip-mall lawyer with 22 wins against some judge was necessarily better than white-shoe law firm that won less often against the same judge. (Although in most companies I have worked for, if the case even got to trial something has gone wrong- everyone pushes for settlement. So judging by trial win record is silly for a second reason).  
>
>I deal with mathematical models for a living. I always validate the model by checking them against reality where I know what reality looks like. And I never trust them far outside the regions they’ve been validated. The whole idea of “look, I made a simple model and when I put in hugely unrealistic numbers I get this weird conclusion! Guess that weird conclusion must be right!” is repugnant to me on a THAT IS NOT HOW YOU USE MODELS level.  

# Productivity advice

>Unplug your router when you want to get work done.  
>
>Designate specific times for “dicking around on the internet” and count all this meta-talk about productivity as dicking around. 
>
>Block your favorite time wasting sites during your working hours.  
>
>Listen to specific music while you work, so that eventually putting that music on will help get you into work mode.  
>
>Take inventory of what is helping and what isn’t- if you’ve been active in a community revolving around “productivity hacks” and you haven’t gotten more productive, stop participating. 
>
>Also, when you encounter something new you are interested in, you might consider disconnecting from the internet, TV and the like and going somewhere with a new book and some paper to take notes.  Once you get over that initial hurdle maybe you can keep the momentum going. 

# Bad mathematical pedagogy

>You got 5 hours of teaching instruction?  We were told to prepare one 10 minutes lecture, which was tape recorded, and then an education grad student watched the tape of the lecture and gave us notes.  Half hour, total.  Also, in the room I gave my lecture in, the mic on the camera was broken, so the notes were like “you look animated, so I’m sure the stuff you were saying was good.” 
>
>Also the bad pedagogy starts so early.  When I tutor kids, I’ll hear things like “oh, my math teacher said this is just a thing you memorize” or “my math teacher says you need to just memorize all the tables of derivatives because each one is special.”  (And I’ll check with the teacher, who will confirm they told them these things).  Math taught as a bunch of “memorize all this shit,” instead of playing around until you get answers. 
>
>Then they get to college and the most important math classes for physics (calculus, essentially) are taught by people who never use calculus (mathematicians).  The intro physics courses are usually a mess of trying to serve all masters (engineering students, physics students, bio students,etc), and then by the time students get to mid-level physics suddenly they are expected to perform on much harder material, using techniques no one has taught.  
>
>For E+M, I’d say Purcell is the best intro (K&K level) textbook.  I think Heald and Marion is better than Griffith’s E+M book (which I think is better than his quantum book but still not great).  
>
>For stat mech, Sethna’s book Entropy,Order Parameters and Complexity is a favorite of mine, and it’s free online http://pages.physics.cornell.edu/~sethna/StatMech/ 
>
>For hamiltonian/lagrangian methods in both E+M and classical mechanics, Landau and Lifschitz are fantastic but there aren’t exercises and you really need a decent intro knowledge to get much out of them.  
>
>For some useful math methods stuff, I really liked Stone and Goldbart, which is also available online (here for calculus of variations,function spaces, and a bunch of diff eq methods  and here  for differential forms, topology, Lie groups).  
>
>At one point in grad school, my friend’s roommate was an algebraic topologist and she was teaching calc 3.  Her problem sets were totally devoid of neat applications, and were just things like “do this multi-integral,” “take this gradient,” “calculate this divergence…”  And because she didn’t really encounter these sorts of problems with any regularity in her work, she didn’t have any real added insight vs. the textbook. 
>
>She was bored teaching it, the students hated it, and what is worse they came out of the class completely unable to set up a simple realistic multi-integral problems (i.e. the electric field of a point charge is 1/r^2 and points outward radially, calculate the electric field a distance z away from an infinite plane of charge), etc.  
>
>On some level, I understand the need for rigor and I appreciate consistent models with a few axioms can be very beautiful.  However, I would have no business teaching analysis - I find it’s a lot of tedious definitions because most of the time in practical problems I’m thinking about things in terms of infinitesimals anyway.  The person teaching it should some deeper appreciation for analysis.  
>
>Come to think of it…It’s pretty fucked up that nearly 100% of the people teaching pre-highschool mathematics probably don’t actually have any feeling for what math actually is.  
>
>If a person finds themselves enjoying reading about Bayes theorem and logic puzzles, prisoner’s dilemma, etc then they probably do in fact like math.  They just don’t know how to connect math the symbol manipulating thing to these mathy ideas I actually enjoy.  
>
>And thats totally on us where "us” is math people.  Most math teaching is sort of terrible, especially pre-college but even in college.  I often think it’s a bit of a travesty that calculus is taught by the math department and not the physics department. 

# Preparation for undergrad

> People enter undergrad with vastly different preparation.  If you feel less prepared than others, you probably actually are.  It can take work to close that gap!  
> 
> >Students can transfer to my institution if they have above a certain GPA at community college. A lot of the community college students (maybe the majority I’ve taught) have been driven and very smart. But they were lied to over and over about what preparation for upper division physics entails, and they arrive and they see the gap between where they are and where the rich students in the super special selective “you had $200/hr HS tutoring” group are and they just give up.
> >
> >Their TAs get mad at them for not knowing what a Taylor series is. The university doesn’t do entrance/placement tests and even if they did, they pressure the CC students to graduate “on time” a great deal.
> >
> >“We should get rid of guaranteed admission from community college,” says a prof. “It lets in students who don’t care and are just not smart enough to do physics.”
> 
> And it’s not just CCs that lie about what adequate preparation is.  My undergrad had separate physics for engineers and physics for physics majors, but my graduate school did not.  It only had the Halliday-Resnick physics for engineers class- which was not adequate preparation for the physics majors.  
> 
> They’d get to the end of sophomore year or the beginning of junior year and all of a sudden we stop coddling them- they have to do integrals, solve differential equations, etc.  
> 
> I got into big arguments with the program director pushing for intro courses for physics majors.  A ton of people hit the end of their second year and found themselves scrambling for a new major because it turns out they didn’t like what physics actually is- we spent a year and a half hiding it from them. 
> 
> Also, the overwhelming majority of the people who say “I never study” usually do, at least a little.  It’s a weird game people play- “look how little effort I’m putting in!”  
>
>Sometimes even if you can explain what you are working on there is so much background material that it just takes a long time.  A friend of a friend once was (for some reason) seriously interested in the Higgs boson, and asked me to explain, and after a long back and forth of questions and about 5 STRAIGHT HOURS of explanation we’d successfully gotten as far as “this is a boson.”  I think it would have taken 5x as long to get to the idea of a Higgs mechanism.
>
>Like,I’ve written 10 or so quantum posts of just background info that haven’t really even gotten to what I want to cover. I will try to keep it light, and not overly math filled- which means I’m not really teaching you physics. I’m teaching you some flavor of the physics. I originally wrote here “you can’t expect to make ice cream just having tasted it,” but I think a better description might be “you can’t expect to make ice cream just having heard someone describe what it tastes like.” There is a sort of uncertainty principle between clarity and truth. I can say something that is sort of true, but very clear or something that is hard to follow but accurate. 

# Taking something seriously means criticizing it

>I generally think it’s a strong point against something if there are no serious critics. Taking an idea seriously involves taking it apart, pointing out where you think it doesn’t work, extending it in different directions,etc. Bayesian stats is an idea I take seriously- I’ve spent a lot of my working time thinking about where I should be using it and where I shouldn’t,etc. It’s an idea a lot of people take seriously- there is a literature full of paradoxes. In the culture I was trained in (physics), if no one is criticizing you, no one is taking you seriously. 
>
>With friendly AI, even most of it’s handful of big-name proponents don’t take it seriously in the sense of engaging with the technical work.  Hell, I’d bet most of it’s proponents don’t read the technical work, don’t have any interest in reading the technical work,etc.  
>
> >I don’t like the idea that, when an author says, ‘what you’re doing really fucking hurts me, and it would mean the world to me if you would just not talk about my work,” “lol watch them have a public breakdown lol they can’t take criticism’ is a remotely appropriate response.
> 
> But this is a fine line, if no one criticizes, how does anyone get better at anything?  Insightful criticism is showing the readers of the criticism (and maybe the original author, if they read it) how to do it better.  
> 
> The only things I’m actually good at in life, I got good at because of criticism.  Theoretical physics, which is my training, is very rough and tumble and most of a graduate students ideas ARE terrible.  You learn to distinguish the useful ideas from the silly ones by having lot of people explain why you are wrong about everything all the time.  The least productive departments are the ones that are too small to provide that kind of critical environment.  
> 
> Also, several years of dealing with reviewers (with my adviser helping me determine which reviewers were actually giving good advice, and which reviewers should be ignored), I got much better at academic writing.  
> 
> I occasionally design origami as a hobby.  In college, I brought some of my (I thought amazing) designs to a hobbyist club, and was told that most of it was absolute shit (because it was).  The proportions were all wrong, and nothing was particularly interesting as abstract forms, etc.  BUT, the same people were able to show me how to achieve a better look, and simpler methods of design then what I was using.  In trying to improve my designs, with a bit of guidance, I learned more about origami design in a few months then I had learned in several years.  
> 
> A friend of mine, and former roommate, is a professional editor for film and TV.  He spent a lot of his free time in college watching really bad movies, and he often says you can learn more by watching really bad editing then really good editing.  Really bad editing calls attention to whats its doing wrong, while you never even really notice the good editing.  Its easier to see how something is constructed when the seams are showing.  
> 
> I think the worst thing that can happen to an artist’s growth is to wrongly overestimate the quality of their work, and to stop striving toward improvement.  

# Real world machine learning ethics

>Sometimes hospitals act as the primary provider for their own health insurance plan.  When this happens, there is supposed to be a metaphorical wall between the hospital as employer and the hospital as insurance company/health care provider.  
>
>People’s personal medical problems aren’t supposed to float from the health care provider side to the business side because otherwise they can start making calculations like “So-and-so employee has a rare chronic illness with a drug that costs $4k a month.  We can save nearly $50k a year by firing her and hiring someone who won’t use as much health care.”  There are laws (HIPAA) about this sort of thing.  
>
>Basically, I think the real machine learning/AI ethics problems these days are about who owns data/what can we do with data.  
>
>Another example- a friend of mine did a model for a company to find employees that were loyal enough to not quit if their benefits were taken away.  This was in an attempt to phase out a defined pension plan with minimal business disruption.  

# FizzBuzz is useless as an interview question

Sure enough, I've never encountered this question in an interview. 

>I’ve been interviewing people for awhile now, sometimes for low level data analyst (no programming experience necessary “some excel” required) positions and sometimes for data science positions. In several dozen applicants I’ve literally never had anyone miss this question.  
>
>I don’t understand why my sample would be so skewed- I’m thinking maybe this idea of people with CS phds who can’t write simple code is incredibly exaggerated. To be fair- we almost certainly do get people who probably couldn’t pass such a test, but they never seem to make it to the actual interview stage.  
>
>I mean, I hear “lore” that a lot of people can’t do this. But I’ve been interviewing people with diverse backgrounds for a variety of positions (none of which are full time programming/software engineering) who can all do this. I’m going to remove it from the interview questions because it has no value as a question. 

# In favor of methodological pluralism 

>It’s important to remember that “LW style rationality” is a large, somewhat nebulous target.  There is a lot of stuff in there about avoiding mental biases, thinking through problems,etc.  This is what I might call the “Kahneman sequences.”  I think these are pretty decent pop-sci although not so much better than just reading Kahneman.  I’ve read a lot of the good judgement project papers (and a relative was a super forecaster) and I think a lot of the “secret sauce” follows this sort of thinking- what Kahneman called reference class forecasting,etc.  I don’t think this is explicitly Bayesian- use whatever method works, but I do think this is good and useful stuff. 
>
>Now, apart from the Kahneman sequences, there are also what I think of as the “Bayes sequences” that present mathematical arguments that Bayesian probability is the only way to think.  I think these sequences are actually counterproductive to developing a mindset described in the Kahneman sequences- keep your models small.  If there is an obvious case for using frequentist statistics, use it (here I’m thinking particle accelerators, which do tons of the same experiment every single second, i.e. particles slamming together).  If there is an obvious case for null-hypothesis testing, use it (i.e. the standard model vs. new physics in a particle accelerators).  If Bayesian updating is too brittle for the data streams you are trying to combine, try Dempster-Shafer,etc.  Ignore the tao-of-Bayes for the tao of the pragmatic.  I also think the mathematical arguments fail for technical reasons. 
>
>There are also what I think of as the “Bayes vs. science” or BvS sequences.  These include the quantum mechanics sequences.  I think this set of sequences are just failures, but it seems like nobody reads them anyway so I won’t go into too much detail.  
>
>There are also pieces on evolution, pieces that read like “mysticism/inspiration for atheists,” anime influenced self-help, cryonics/transhuman advocacy, political philosophy, pieces on AI, etc.  I’m not sure there even is a core group of beliefs.  Some rationalists have even told me that I myself am somehow a LW style rationalists despite my somewhat large disagreements because I do like the Kahneman literature. 
>
>What I’ve been trying to argue is that the scientific community (which has been very much executing a sort of genetic algorithm for a few hundred years now) is currently a much better “lens that sees it’s flaws” then the Bayesian/rationalist program.  And the lesson there isn’t standardize, standardize, standardize- it’s run in a zillion directions at once.  At most conferences I’ve been to a plurality of physicists each thinks that every other physicist is doing it wrong.  And that is a good thing! 
>
>And the big lesson of the scientific program has been methodological pluralism.  We have statisticians working on methods (some of which are Bayesian and coherent, some of which are frequentist bounded, some of them desire to explain, some desire to predict), we have physicists working on principles of a deep reductionism trying to find the fundamental building blocks and other physicists who laugh at them and say “why bother? It’s all emergent all the way down, help us figure out this new weird state of matter.”   
>
>The current “crisis of replication” plaguing some social sciences is what happens when that pluralism slips and one method starts to dominate a field.  Use any method that seems reasonable and informative, not just simple null hypothesis tests.   
>
>Edit: and of course we don’t know if this is the absolute optimum.  And I don’t think we can ever know.  In very, very simple business decisions I usually can’t show a method is optimum, no one can.  If no one knows how to prove optimal methods for figuring out well defined insurance problems given nearly infinite amounts of data, why should we assume that there is any provably best method for doing anything complicated?
>
>Consider the humble p-value and null hypothesis testing.  Now, in Less Wrong and a lot of places, you’ll find a lot of criticisms of the p-value.  It’s easy to game to squeeze a publication out, it has become a defacto criteria for publication, the 0.05 cut off is arbitrary,etc.  But the big point is that by making it a systematic requirement,  you are creating a systematic incentive for bias in the results.  
>
>Now, Less Wrong would generally say “well, that is why you need Bayes.”  But my point is that in a world where Bayes had become the defacto norm, people would just be hacking Bayes factors.  Our results would still be biased, but differently so.  
>
>The solution isn’t to switch from hypothesis testing to Bayes, it’s to get rid of the systemization.  Let the Bayesians Bayes and the frequentists signal test.  Maybe you don’t always need an 0.05 p-value.  Maybe a big potential signal that happens to be 0.06 significant is worth reporting so that people can expand the study, try to narrow it down,etc.  
>
>I’m not opposing the very idea of systematization, I’m opposing the idea of a single systematization because I think it will do worse. 
>
>There is a quite general variance/bias trade off- any systematic, low variance model will have some bias.  So if you insist that everyone uses the same method then you are building in the same bias.  
>
>What you need is the realization that there are going to be multiple correct methods, so the best thing to do is run them all in parallel. 
>
>So my fear is that if you try to codify the way individual researchers should think, you build in bias.  
>
>Like, imagine an alternate history where scientists are Bayesians Imagine cosmic observations are relatively easy to make (maybe some chance let people invent lenses earlier) some young Isbert Newtstein through careful observations lays down general relativity 
>
>Now, more and more evidence rolls in GRs favor, and it keeps getting a higher and higher odds ratio of being correct.  Meanwhile, confusing observations regarding small particles start piling up.  Schroedinger tried to write down a general equation of the behavior, but the only relativistic version gave the wrong energy levels.  
>
>He decides not to test the equation we know as the Schroedinger equation (SE)because he computes it’s consistency with GR, since P(GR & SE) = 0, then P(SE) is proportional to the probability of GR being wrong, which is near 0.  
>
>Years later, some observations of dark matter knock down P(GR) a tiny bit, and nothing else has worked, so finally it’s worth testing Schroedinger and it seems like it’s working.  And then alternate Heisenburg and Dirac start to suggest a general theory, where you quantize a Hamiltonian.  But again, P(GR & Quantum) is so, so small that it’s not really worth promoting to anyone’s attention.  
>
>Basically, if everyone is following the same set of “rules” I think you build in bias that can be really harmful.  You need a large community all trying different things- some people really are fairly Bayesian in the way they work on new theories, some people seem (from my perspective) to be taking an essentially random walk in hypothesis space.  Some people are guided by what they think is cool mathematics,etc.
>
>These sort of observations have very much informed a lot of my approaches to problem solving- try everything you can think of and see what worked best this particular time.  I honestly think this idea seems really foreign to Less Wrong, which is probably why I’m having such a hard time communicating it. 
>
>I’ve previously argued that in real situations Bayesian thinking isn’t always optimal, coherency with previous evidence might be much less important than overall explanatory power.  You might not be able to effectively enumerate a sample space,etc. 
>
>If you convince everyone that coherency with existing evidence is the most important thing, then you dramatically narrow the hypotheses being considered. 
>
>But this isn’t unique to Bayes- let’s say instead of coherency, you convince everyone of a specific form of Occam’s razor, or specific ideas about naturalness,etc are the proper way to explore science.   This would also create the same biases. 
>
>For instance, in a non-Bayesian context, I think a big weakness of particle physics is that most of the recent models were developed with an eye toward naturalness.  I think the idea that only renormalizable field theories mattered set back the understanding of effective field theory,etc. 
>
>I’m arguing against the idea of one “meta method” for exploring “idea space.”  You need a diversity of view points.  Some people can and should be Bayesian, some might be motivated by naturalness concerns, or the fact that they think algebraic geometry is the most elegant form of mathematics,etc.  

# Work anecdotes

>My underling was super pumped about a model he made with 100% accuracy (too much accuracy is a sure sign you messed up).  In an attempt to be a bit gentle, I wrote him an email that said “Hey, take a look at some of the other models that are running in test, you can get their validation metrics…” 
>
>Basically, I asked him to do a gut check on whether 100% accuracy really makes sense in a world where everyone else is working hard to push up above 80%.  
>
>He just wrote back “maybe my model should just skip test all together and head straight to production, I’m regulating and dominating!” 
>
>And I just dug in a bit in his model. He tested the model on his training set. The new amazing predictor he discovered? Row number. 
>
>After a talk about basic things you can do to optimize code:
>
>Actuary: “I don’t understand why you are spending all this time talking about optimizing.  Our process works, that is good enough.”
>
>Me: “You know that pricing process that takes two hours?" 
>
>Actuary: "Sure.”
>
>Me: “As one of the examples I was thinking of doing for this talk, I moved it out of excel and into the database.  Now it takes less than 10 minutes." 
>
>Actuary: "What? No? What? Really? FUCK”

# Undergrad research experiences

>I did research as an undergrad, so I’ll tell you about that first. Then, I’ll talk about some other people’s experiences.
>
>My program had a requirement that we take a programming course for physicists in our first year. I hadn’t ever coded before I took the class, but I did pretty well in it. The prof was really impressed with me for some reason. He’s a really nice person.
>
>I had pretty shit marks in first year and the first half of second year. In 2nd term of year 2, I did really well and took another class from this prof over the summer (Quantum 2) which I also did really well in. I spent year 3 knocking out degree requirements (Quantum 3, Stat Mech, Condensed Matter, etc.) and applied for an undergraduate research grant from NSERC (Canada’s NSF equivalent). I got the grant, which paid me to do research over the summer, and I did it with this prof.
>
>He wanted to look at using GPUs to accelerate exact diagonalization methods for spin systems. I had one term of C++ programming at this point but for some reason he hired me. I was very bad at C++ at this point so I had to teach myself how to do it at an ok level as well as teaching myself GPGPU. Somehow this worked, and I had a working code by the end of the summer that was also faster than the CPU version. Hell yeah.
>
>My university had an optional 2 course senior thesis which was recommended if you were applying for grad school, which I did, continuing this project and applying the ED code as part of a bigger algorithm. I did that through graduation and over the summer before grad school.
>
>Day to day, what undergrad research was like for me was reading the CUDA C Programming Guide, being really confused, slowly getting better at pointers through prodigious use of gdb and printf, and going to a lot of talks. My energies were wrong, wrong, wrong, NaN, wrong, wrong, oh holy fuck they were right, oh no wait, they were wrong again. I must have annoyed the hell out of the grad students in the 10 person open office I was in (i got a desk in the GRAD STUDENT OFFICE I was SO EXCITED). We got coffee a lot and drank a lot of beer. No one else knew how to GPGPU so I was kind of left to my own devices to figure everything out. Some bugs took me way, way too long to fix because I’m real dumb.
>
>My undergrad is famous for putting people into internships very early. I knew lots of people who were interning at Google and Microsoft in their 1B/1C terms. Depending on what team you get put on this may be “research”. I knew a few really good kids in CS in the Honours programming sequence who got pulled into prof research groups directly as well. That’s how it worked in CS, unless you did really well in 4th year graphics and got hired into one of the graphics mega-groups.
>
>Now that I’m in grad school, I’ve seen a bunch of the students I’ve TAed try to do research. They’ve had varying levels of success. One thing undergrads don’t realize, I think, is that TAs gossip about you a lot. Everyone knows That Fucking Kid with the handwriting who turns his homework in on a bizarre shape of paper. Similarly, everyone knows who the “good” and “hard-working” students are. That impression is formed from how well you do on homework and how good you are at answering questions in section. Of course it’s possible for you to be a good and/or hard-working undergrad and not show it in these ways but it’s harder for your TA to see. When a student comes to a prof asking to do research with them, it’s quite common for the prof to consult their grad students. In most cases undergrads are a time and money sink for the group so the prof is hiring you as charity (because they want students to go on to grad school, because they did research as an undergrad, whatever). So if your TAs like you, it’s easier to get a spot. Suck up to your TAs and have nice handwriting or LaTeX your homework.
>
>Getting into a theory group is really hard as an undergrad. You can either be good at coding or good at QFT. Being good at coding is easier because scientists suck shit at coding so it’s easy to look good by comparison. Learn Python, C++, or FORTRAN. I really like Julia but 99% of profs don’t know it. Don’t learn MATLAB if you can avoid it. Fuck MATLAB. If you get a coding research position expect to either have very little guidance or being someone’s script/debugging monkey. Being a monkey can be valuable because you’ll know at least one technique going into grad school.
>
>There are good and bad experimental jobs. Many profs have an army of soldering slaves who don’t learn anything else. That’s cool, in that they’ll probably write you a decent letter and you might get paid, but bad, in the sense that you want to learn skills beyond soldering so that you can either get an industry job or be at the front of the pack in grad school. My impression is that cold atoms/AMO/shit with lasers labs teach you more but the grad students will sneer behind your back because you will take 5x longer to do anything than they will. You will also have the capacity to break extremely expensive equipment very easily. Undergrads who get paid $5000 for the summer have a nasty habit of breaking $20,000 worth of glass. Don’t be that kid. Also, don’t be the kid who leaves their fingerprints on the lenses all the time. I’m not an experimentalist but all my CMX friends complain constantly about lenses fingerprint kid. One of my friends put his hand in front of a 10W green laser at one point as an undergrad - don’t do this either, it feels like a 10,000 C knife going through your flesh. He still has a burn pinprick mark on his hand.

# It's not worth worrying about "AI risk"

>The process of developing a “superintelligence” will necessarily lead to greater understandings of natural language processing, AI, etc.  This understanding is foundational to questions of the danger of AI, and it hasn’t been done yet.  So the “danger” argument relies on a lot of areas where we don’t know the answer and we don’t even know enough to put a good “Bayesian estimate” together.  
>
>i.e., yes you can recreate a synapse in silicon but it’s inefficient (as it’s an emulation).  Does the ability to toss more processors in overcome that?  No idea, there are limits to parallelizability (Amdahl’s law,etc.)  Will training an AI in a machine learning environment with human-built-examples lead to values similar to humans?  How large is the space of possible minds? All of these are unknowns. 
>
>Therefore, any work on narrow AI, natural language processing, machine learning,etc is work that is advancing both AI and “AI safety” research, by answering these technical questions.  
>
>Basically, the proper first course of action is to answer the technical question required to estimate AI safety and people are already on that. 
>
>If you want to talk about the moral weight of a fundamental particle, you should work to clarify not mystify.  I don’t see how his argument is any different than “we don’t know what suffering is so maybe fundamental particles can suffer.  Also, there are a lot of fundamental particles.” 
>
>I find this argument structure incredibly tedious:
>
>1. find some area of scientific/philosophic uncertainty
>
>2. jam in a ton of assumptions without really noting them specifically (”utilitarianism” smuggles in more than people think.  He is also smuggling in some sort of direct additivity, assumptions about how utility functions ought to be built,etc.) 
>
>3. Use insanely big numbers to overcome the uncertainty. 
>
>i.e. he could have stopped by saying “do grains of sand or rocks suffer?” but took it all the way to fundamental particles for the sake of extreme big numbers.  
>
>This is how the AI-risk argument works, for instance.  It doesn’t clarify anything, no one learns anything, it’s just “look, if we pile a bunch of assumptions into this argument from ignorance we can come to extreme conclusions.” 
>
>It’s possible the universe is in a false vacuum and a high energy experiment could knock us out of the false vacuum into the true vacuum (thus, this entire universe will be destroyed by a bubble expanding outward at the speed of light). 
>
>It’s possible some biology grad student will accidentally breed a super virus while trying to do something or other and kill us all.  Or maybe an evolution experiment creates a transmissible cancer that wipes us out. 
>
>Maybe chemists can create ice-9 by accident and kill us all. Maybe they can create molecular nanotech and grey goo the world,etc. 
>
>Maybe,maybe,maybe.  It’s all just arguments from ignorance.  You could write the same sequence post and title it “should we ban AI” and stick in AI references instead of physics references.  The unknown can kill us, should we keep striving towards the unknown? 
>
>There is a theological position called “God-of-the-Gaps,” a type of argument from ignorance, where you push God’s interventions increasingly into the gaps in scientific knowledge.  God was involved in keeping the solar system stable until we learned more about stability and chaos in dynamical systems.  Maybe now God intervenes in brains, or in abiogenesis,etc.  Wherever science has the greatest uncertainty, you can stick in God, because, hey, it hasn’t been ruled out.  
>
>Bostrom’s book is basically “AI-risk of the Gaps.”  In the handful of cases where explicit pathways forward are clear, Bostrom concedes that there may be less risk of a “fast takeoff”- this is especially true when he discusses simulation evolutionary environments for intelligence (he suggests it would take more than a century of Moore’s law doublings to arrive there), and when he discuss biological and mechanical augmentation of humans.  
>
>With whole brain emulation, Bostrom tells us we’ll be able to see the advances in technologies like high-throughput, high-resolution microscopy.  We’ll see this on the horizon,etc.  
>
>The real danger Bostrom tells us, is machine AI.  But the reason it seems dangerous is because it’s much less defined.  Maybe machine AI will take a dedicated research team doing slow, incremental progress.  OR, maybe some hacker in his basement will invent AI tomorrow and kill us all (because, hey, it hasn’t been ruled out.)  The process is uncertain, so we are free to imagine whatever scenario we want, including very dangerous ones.  
>
>A superintelligence, we are told, might be so much smarter than humans that the relationship won’t be like Einstein vs. the rest of us, but rather it will related to us the way we relate to plants.  How does Bostrom know?  He doesn’t, it’s uncertain so he can put as much risk as he wants there because, hey, it hasn’t been ruled out.  
>
>And it’s telling that he makes zero attempts to qualify how intelligent a “super intelligence” might be.  In a several hundred page book about how powerful a computer system might be, computational complexity is mentioned ZERO times.  The fundamental unpredictability of chaotic systems?  Zero times.  Areas like these where we have solid mathematical results to help us try to figure out what problems might be tractable vs. what problems aren’t are totally ignored.  Instead it is imagined that a superintelligence is a wizard with unlimited power to do whatever it wants (because, hey, it hasn’t been ruled out.)  
>
>The problem is that this isn’t grounded in any realistic conception of an actual system (occasionally, he makes handwaving mentions to literally uncomputable ideal Bayesian agents).  No one knows how such a strong-AI might work, so you can pile any assumption you want about the risks.  
>
>Will a take off be fast or slow? Well, with machine AI it could be fast, because hey it hasn’t been ruled out.  
>
>These assumptions often seem quite wild, but hey, it hasn’t been ruled out.  As one example, Bostrom suggests we could build machine with the goal to build 100 paper clips.  But, because it’s an ideal Bayesian, it will never give 100% probability to the hypothesis “I’ve built 100 paper clips.”  So it might use it’s mighty power to turn the Earth into a giant counting device that repeatedly counts the same 100 paperclips over and over.  But why stop at the Earth, it could take over a region expanding at the speed of light to turn as much of the universe as possible into a machine that counts the same 100 paper clips over and over. 
>
>It seems ludicrous to me that a system that can’t tell how many paperclips it has previously manufactured will somehow be able to accomplish other tasks. Such an agent will put non-zero weight on the probability that any conceivable action will prevent it from building any paper clips at all.  Faced with even a tiny chance of failing at it’s goal, wouldn’t it make no moves at all?  It seems ludicrous to imagine we’ve cracked the problem of human-level intelligence but the resulting agents can’t count paperclips.  But hey, it hasn’t been ruled out.  
>
>And that is why reviewers say the argument seems like a lot of suppositions, caveats, maybes, all piled on top of each other in an unsatisfactory way.  Bostrom is finding those areas where we don’t really know how these things will work, and he is conjecturing about the worst possible outcomes.  When he talks about how to control these things, he can shoot down any control scheme the reader can think of, because he is free to reimagine how the AI might work to get around such a control scheme,etc.  It’s all just supposition built in the gaps in our knowledge. 
>
>It’s important for me to be clear about my position here- I’m not saying that smarter than human systems can never be built.  On the contrary, it seems very probable that they can be. However, there are obvious constraints on how intelligent a superintelligence might be.  Computational complexity limits our ability to solve even well defined, well understood problems.  Chaos limits our ability to predict the future even when we have a complete understanding of a physical system.  Well known mathematical results in fields like optimization and linear programming put hard constraints on how effectively a system can be optimized along multiple axes.
>
>A serious attempt to understand the risks and rewards of superintelligent AI needs to grapple with these limits, instead of finding the gaps in our understanding and insisting those gaps contain existential risk. 
>
>Sure, we don’t know what an AGI would optimize, this is because we have no idea how an AGI might work.  It’s by generalizing from existing successful learning methods that we’ll move forward, not by taking wild shots in the dark.  For problem-specific AI, satisfactory outcome is usually relatively simple to define.  The problem is MIRI is saying “we need to formalize a satisfactory outcome for inventing God” and that is pretty ill-defined.  
>
>We don’t know what recursive self improvement looks like, until baby versions that do something useful are invented there is nothing here to study.  
>
>And do you really think we’ll get anywhere near making an even human level general intelligence without better understand “what counts as general intelligence?”   To study these things, we need to have some understanding of how to build these things, and we don’t.  In figuring out how to build these things, we’ll learn the answers to these questions.  
>
>So we don’t have a full understanding of deep learning, but the physicist in me says it’s “just” an implementation of Kadanoff’s variational renormalization group.  
>
>The corrigibility problem seems to me to be very specific to the details of the implementation of the system in question, and isn’t particularly well defined.  
>
>As for “we have no reliable means of writing software that does just what we want the first time we run it”- not yet but the formal verification people are working on it.  For certain types of mission critical software these processes are already in use.  
>
>I see the path to “safe AI” to run from continued work in understanding narrow-AI (more work on deep learning/RG as autoencoders, more work on generalizing well from evidence, more work on selecting proper targets for supervised learning), and from work in critical systems (formal code verification, robust failure,etc).  None of this seems to be the work MIRI wants to engage with.  The statistical approach to AI/learning has been tremendously effective and it’s where progress is being made.  
>
>I don’t think the formal mathematical approach to AGI will yield much that is interesting, but even there Marcus Hutter’s group seems to be doing more to advance our understanding than MIRI.  
>
>Where I see concerns for safety, I see existing AI researchers working on the problems.  
>
>Here is an example- MIRI talks about the superhuman AI that is told to maximize happiness and instead ends up tiling the world with smiley faces.  If you know a bit about machine learning you’ll recognize that this is a problem of over fitting/bad generalization.  The machine learned a true thing, but now what you wanted. 
>
>But this is pretty much the core problem of machine learning, the only thing MIRI did was “imagine an overfitting case that could end the universe.”   Everyone is working on this problem!   There are probably hundreds (maybe thousands) of papers on regularization, robust statistical estimation techniques, small biases toward low-risk decisions to reduce the variance of estimates,etc.  Do you see my point?  It’s hard to imagine a situation where we get to “super human AI” without having a really solid understanding of the dangers of overfitting, given that we have to deal with these problems just to get an algorithm that correctly predicts insurance claims.  
>
>I think mainstream AI is already working on making a computer understand english language commands (genie problem) and mainstream AI is already working on making sure that when you train a model it learns what you want, and not something stupid (wirehead problem),etc.  The reason people work on these problems is that people see paths forward,etc.  
>
>Next to no one is working on “invent a super powered reasoner to solve all the worlds problems.”  
>
>I don’t think anyone with a reasonable knowledge of the field will think that the latter will be solved before the former.  It also seems pretty likely solving the former will be necessary to solving the latter.  
>
>1. two of the points Scott brings up are already considered big, foundational problems in current AI research.  The “evil genie” problem is really a problem about communication with the AI- you request something and follows the letter, but not the spirit of the command.  
>
>The wireheading thing is a classic example of overfitting.  These are common problems dressed up in AI-risk language.  It’s silly to assert that these are problems MIRI discovered, and it’s silly to assert that researchers aren’t working on them and need MIRI to point them out.  
>
>The pascal’s mugging thing maybe shows why you shouldn’t use unbounded utilities and crank through simple EV calculations.  But there are many obvious ways to avoid it (add a tiny bit of risk aversion, or bound the utilities,etc), many of which you’d want to build in anyway.  It’s also probably not the way anyone will build any type of AI- the revolution in modern methods was statistical learning, not trying to build perfect reasoning machines.  
>
>2. I’m not sure I made this point, but I think it underscores a problem where many of the people talking about AI risk seem to have basically no contact with the mainstream field.  When they say “I care about AI risk” what they really mean is “I enjoy talking about AI risk on the internet” and not “I care enough to read up on current AI methods to really try to understand the risk” or “I care enough to follow MIRIs technical research.”  
>
>And again, my point isn’t that you get perfect goal alignment via properly processing language.  My point is that you get enough clarification that it’s not going to do the obviously stupid things.  It’s not going to get your intent completely wrong.  
>
>The problems postulated along the lines of these sorts of “evil genie” problems are failures of communication not failures of values or something like that.  You intend one thing, but rather than reading your intent, the AI plays “rules lawyer” with your sentence.  I posit that an AI that is really programmed to understand human speech, in ambiguity and context, isn’t going to have that issue.   
>
>My claim is not that it’s a solved problem, my claim is that understanding what people intend with speech is the natural language processing problem.  And the natural language processing community is making a ton of good progress, for reasons totally unrelated to AI risk.  The work unrelated to MIRI has done infinitely more to solve this problem than MIRI has without even being aware of MIRI.  
>
>It’s not a question of sharing your values, it’s a question of understanding what you mean when you say “create world peace.”  You obviously don’t mean kill everyone, wirehead everyone,etc.  
>
>This problem Scott refers to as “wireheading”- i.e. the AI manipulates it’s reward function and so instead of doing what you want it ends up doing nothing useful at all.  
>
>This problem is literally straight out of machine learning 101.  It’s just overfitting in disguise.  Giving a machine a data set and it learns something totally valid but totally useless is incredibly common.  
>
>I once made an algorithm to try to find risky insurance claims, and in my training set there were two types of entries electronic entries, which came in monthly and manual data entries.  Now, the reason we had manual entries was that there are auditing requirements for super risky claims, and so sometimes finance had to input data before the monthly automatic data load, in order to meet some regulatory deadline.  But, when people put in manual entries, they make mistakes, they round numbers, etc.  So what my model learned from this data was essentially “if the totals don’t add right, it’s a risky claim!”  Totally true in that data set! Totally worthless!  
>
>How to set up a proper model and a proper learning function is the central machine learning problem, and basically the entire machine learning community is working on it.  Like the ambiguous-goal problem I discussed previously this is also a problem already being worked on for pragmatic/practical reasons by the existing AI community.   
>
>The issue is that you are assuming it has this super powerful intelligence that it can use to infer intentions and understand the world around it, but for some reason it isn’t going to be able to reflect on it’s own goals. 
>
>My larger point here is that we already have people working on inferring intention from ambiguous statements, and that is the line of research needed to solve these sorts of problems.  The relevant research is being done for unrelated reasons already.  The people who build the future AIs will have access to all this research and will be building off these methods. 
>
>Imagine an AI that learns a utility function the way a human does- we’ve been parenting for a few hundred thousand years and still don’t know how to teach a human in a provably safe way.  
>
>Until you know how an AGI actually will work you can’t do anything to “prove” safety.  You don’t even know what the risks actually are until you know what exactly it is doing.  
>
>Here is the problem- There is a really huge gap between today’s machine learning algorithms and AGI.  There is a really huge gap between today’s biological simulations and whole human brains.  No one knows what will fill in those gaps.  This means that MIRI is attempting to create a sort of best-practices for a technology without any deep understanding of how that technology will actually work.  It’s like if Newton had tried to design safety practices for nuclear reactors.  
>
>You can design a “safe” utility function but it’s worthless if it turns out AGIs aren’t built around utility functions.  You can design a “safe” learning incremental learning algorithm, but it’s worthless.  
>
>If goals/ethics end up tied to knowledge and there are objectively correct answers to moral questions (as utilitarians sometimes believe) then the AGI is already safe because when it gets smart enough it will discover the objective morality.  
>
>If an AGI is just thousands of smart human brains emulated at 100x normal speed, then it will obviously have values similar to humans.
>
>No one knows how these things will work and there is no mathematical proof of “safety.”
>
>Especially because you are defining “AGI” to be “super god wizard that can do anything.”  That is incredibly nebulous.    

# Does collaboration "magnify" intelligence?

>We know that groups working together can accomplish things that individuals working alone cannot.  Think of the Manhattan project, the Apollo program,etc.
>
>How much can collaboration “magnify” intelligence?  I don’t know.  Does the magnification effect increase with better collaborative tools?  I suspect so.  What is the limit on collaboration magnification?  I don’t know, and I don’t think Scott does either.  He tries to create an intuition that a collaboration’s intelligence is bounded by the most intelligent member of the group, but this doesn’t match my experience of collaborations.  
>
>We could have a war of dueling anecdotes, or we could try to put some actual definitions around intelligence/collaborations.  For tasks like classification, it’s often true that ensembles of weak classifiers are significantly better together than a single strong classifier.  This has been an important insight in machine learning (see random forest, for instance), and it’s a simple mathematical model of a collaboration of lesser “intelligences” winning against a single stronger “intelligence.”  This isn’t conclusive “proof”, but it’s at least an actual model, and not hand wavey intuition.

# Incentives in scientific research

>Also the idea that scientists are buying themselves grant money by producing ho-hum support of the accepted “party line” is just nonsense.  The way you make a name for yourself, launch a subfield, etc is to find surprising things. 
>
>The real bias is toward people exaggerating the novelty of results, (consciously or unconsciously) p-hacking to make effect sizes seem interesting and important, etc.  
>
>Now, you still have to show in a grant that your research is well founded and has a high chance of working,etc which does create incentives toward “safe” research.  But this isn’t the sort of thing Moldbug is talking about- it’s not about what the government has deemed “correct” but what the scientific community thinks will work.  
>
>If you are going to bow to incentives and cheat entirely, what you really want to do is have a safe looking research program that demonstrates unexpected and novel results.  If you look at recent cases of scientific fraud (Schon,Hauser,Stapel all spring to mind) I’ll think you’ll find they fit generally into this category- a typical-for-the-field research proposal shows interesting results, so author gets more money to explore the results, continues to find large, interesting effects. 
>
>Prestigious researcher at my undergrad institution was infamous for “helping” his undergrad students write their applications.  
>
>As the incentives to cheat become ever stronger (these days a prestigious fellowship can be the dividing line between having a career/not having a career after grad school) more people are breaking and cheating.  It’s also why so many faked research scandals are increasing.  A lot of the institutions of science aren’t coping well with the new world of job and funding scarcity.  
>
>I speak from experience, I finished graduate school with a very strong publication record.  I won several prestigious awards in my field, and had several postdoc options available. I did a back of the envelope calculation and realized I could make substantially more than a postdoc by bartending and waiting tables while retraining myself to do different work.  So I did that.  I would absolutely love a job in academic physics, but I wasn’t willing to work for less than a bartender for a 1/10 shot of a career in physics after 6 years of postdocs.  
>
>The cream of the crop in my physics cohort (I went to a top school, so this is among the cream of the physics crop worldwide) have all left academic science all together.  We represent millions of dollars of money spent to train us, and none of us put any of it to use after grad school.  We are and were successful scientists who saw that the career as it exists would not allow us to do the science we wanted, so we left for careers that would at least compensate us for all the effort we put in.  
>
>Academic science doesn’t select for talent, it selects for a sort of endurance (an endurance built out of a lack of imagination- the people that stay don’t realize that life could be better if they were doing something else).  
>
>Further, the institutions of science were designed in a time of plenty.  Peer review is a great way to vet information, but a horrid way to allocate scarce funds- and as the funds get more scarce, the corruption is increasing. 
>
>Also, theoretical physics, at least, hasn’t been a growth industry since the 1970s.  Thats 40 years of it being a shitty career.  Yet this information hasn’t filtered to the undergrads- I sat on a panel my last year of grad school for undergrads thinking of grad school.  They were surprised to learn that every member of the panel was leaving science after grad school.  My undergrad institution had no such panel, it was assumed all the physics majors would do grad school, no one thought to give us information about career prospects. 
>
>I also dispute your claim that without competition less science would get done- the golden age of physics, its most productive years, where in a time when funding was plentiful and pressure to produce minimal.  
>
>In the malthusian equilibrium you propose, the least talented people will end up in science, because the alternative careers are so much better.  I have a much simpler solution- get rid of the phd degree.  The phd gives programs an illusion of value “we are providing you with a service.”  Get rid of that illusion and smart people will be less willing to work for $20k a year.  

# Anecdotes about tutoring kids

>I got an email from a student I’m tutoring’s parent who said that her son should not have to do the exercises I asked him to do because he is an indigo child and “he knows what is right for him.” 
>
>Look, if you knew a way to learn this stuff that was “right for you” you wouldn’t be paying me to tutor you. 
>
>Generally, the people who pay me to tutor at least have the mindset that grades are important, if not actual academics.  I do get parents who believe that grades are a matter of throwing their weight and money around.  Once, a parent asked to pay me for a two hour session.  One hour with the student, one hour in which I do the homework straight up, and let the student crib it.  Another parent offered me a decent chunk of cash to “write something sciencey” for a student’s college application essay. 
>
>My bigger problem are kids who are somewhat lazy and don’t want to work.  They hit their first roadblock in a math or physics course and give up entirely.  Their parents are convinced that little jimmy just can’t get it (their poor baby is trying so hard, boo hoo), and when I point out that little jimmy isn’t trying at all, they accuse me of being mean.  
>
>Kid I tutor: “I don’t understand why I did so poorly on the exam.” 
>
>Me: “How many of the exercises [that I gave to do before the test] did you get through last week?” 
>
>KIT: “Well, I only got through 1, but I took a lot of adderall right before the test.”  
>
>Pretty sure adderall can't download intro mechanics into your brain
>
>I always tell students to get it into a form where you have some intuition for what the formula is telling you, so you can check the limiting cases.  But that might be more of an issue for physics than for math. 
>
>Which is why I like (1+2x)(3-4x) a lot more than 3+2x-8x^2.  With the first, it’s easy to tell it’s a function with two zeros, where they are, that it’s positive in between the 0s and negative outside them,etc. 
>
>I often get kids who leave god-awful messes because they took one formula from the formula sheet, plugged it into a second formula, then plugged the whole mess into a third.  So they’ve got a page of garbage in which almost everything would drop out if they took the time to reorganize the expression a bit.  
>
>I fired one of my student and had roughly this conversation with the parent
>
>Me: “I can’t in good conscience keep taking your money.  Your son just isn’t trying at all." 
>
>Mom: "He says he is really working!  He really is trying!" 
>
>Me: "So, I thought he wasn’t looking at the work I gave him, so last week I glued the exercises together at the corners.  It’s still glued. He never opened the packet." 
>
>Mom: "He probably just thought you wanted them back the way you gave them to him.”  
>
>Sure… that’s it…  he glued them back together (without writing anything) because he thought that’s what I wanted…
>
>There is a pervasive attitude, especially among the smarter kids I’m tutoring, that “this is hard and I’m not immediately good at it, why should I have to do it?” (a close compliment of “I don’t see the point in this, why should I have to do it?”)  and it’s horribly frustrating.  
>
>Being able to learn, (and finding uses for things you know/are learning) is a useful life skill.  
>
>“I’m just not a math person, so I’ll never get calculus.”  Well, have you tried any of the exercises I gave you?  Have you read the alternative explanations in the other books I’ve loaned you? No?  Then guess what, you aren’t actually trying.   Yes, it might take you a bit more work than some of your peers to learn this, but I’ve gotten some (frankly) fairly dumb students up to B+/A- grades and a decent understanding of the topic.  Lots of one on one time with an instructor can help like that.  
>
>And if you don’t put the work in, you’ll end up cutting yourself off from huge areas of human knowledge.  A lot of the modern world involves making,optimizing and understanding models. 

# Recommended physics textbook

>Non-calculus based physics is usually absolutely awful.  The problem is that you can’t derive any of the formulas, so everything seems like magic.  “Here are a zillion formulas with little explanation” is generally a terrible way to teach things.  If you are taking calculus (or have taken it) see if you can find yourself a calculus based physics textbook and read through it.  Kleppner and Kolenkow is good, the problems are very hard. 

# Traditional vs liberal values

>A recent discussion that went across my dash between xhxhxhx and nostalgebraist reminded me that I’ve been wanting to make a post about a lot of the talk of traditional values/stable communities,etc that I see described by authors like Charles Murray,etc.  
>
>Oh god, Charles Murray.  In his three most famous books he argued 
>
>1. Black poverty was because of the welfare state (note that a lot of the basic details in this book are misleading or just wrong). Cut the welfare state, and things will improve.  
>
>2. Black poverty is because of IQ differences.  This can’t be fixed, so we better cut the welfare state- its just money wasted.  
>
>3. The rise in white working class poverty is because white people are acting too much like black people.  Better cut the welfare state and get richer whites to scold poorer whites more.  
>
>To get to his positions, Murray ignores huge economic trends (this was as much true in Coming Apart as in Losing Ground) To quote David Frum (I went with a conservative reviewer, for you): 
>
> >To understand what Murray does in Coming Apart, imagine this analogy:
> >
> >A social scientist visits a Gulf Coast town. He notices that the houses near the water have all been smashed and shattered. The former occupants now live in tents and FEMA trailers. The social scientist writes a report:
> >
> >The evidence strongly shows that living in houses is better for children and families than living in tents and trailers. The people on the waterfront are irresponsibly subjecting their children to unacceptable conditions.
> >
> >When he publishes his report, somebody points out: “You know, there was a hurricane here last week.” The social scientist shrugs off the criticism with the reply, “I’m writing about housing, not weather.”
>
>Using Murray’s same data, you can turn Murray’s thesis on its head- and suggest that the collapse in pay and working conditions might have caused people to be less committed to the labor market (this would also explain his data in Losing Ground).  This latter explanation makes more sense to me- after all I left a low paying profession for a higher paying one.  If the higher paying profession hadn’t been available to me, I likely would have become a stay at home husband (I know several former physicists who have done just that). 
>
>My father grew up in very small town midwest, and my mother was midwestern city.  Unlike the majority of my friends from highschool, and unlike the majority of my extended family, I was raised more liberal.  Like all things, this is a spectrum, a midwestern liberal may well be a California republican, but certainly my birds-and-bees talk at home was “wrap your junk” while the sex ed at school was “girls are evil temptresses and you’re better off burning your dick with a hot iron than having sex with any girl willing to have sex before marriage.”  My parents valued education, independence, diversity but also hard work, taking care of your family,etc.  
>
>I wasn’t raised religious, but sort of religious-once-removed.  My father’s parents were very Catholic (he is one of 10), but my father was less so.  The sort of guilt-absolving only-at-Christmas church services and the like.  My extended family range from super-evangelical (speaking in tongues, casting out curses) to strongly Catholic.  
>
>But here is the thing- my parents values, which have more or less become my values have allowed me to survive and thrive in the world we live in, and my extended family and my friends from my hometown largely are not.  
>
>Some of my cousins,living in small town midwest America, because they strongly valued family and their small town community (and because they were probably unbearably horny)  got married out of highschool and went to work at what everyone just calls “the factory” because it’s the only one in town.  Well, it WAS the only one in town, but in the early 2000s the town went from a one industry town to a no industry town.  They’ve scrambled to find comparable work, but it’s required going back to school and trying to learn new skills in their late 30s, early 40s.  And they really, really don’t want to leave their small town community which has left them unable to follow the work.  
>
>Some of my other cousins and some of my friends got a college education, but then moved back to their local, small town community and the end result has been much the same.  I know mechanical engineers who ended up taking community college classes in order to learn how to be tool-and-die makers so they can work for much lower wages at the only jobs in town (which everyone expect will be outsourced within the decade).  I know electrical engineers who apprenticed to electricians,etc.  They tried to live in “the big city” (which is only big by midwest standards), but couldn’t handle the cultural clash.  
>
>My more conservative community oriented friends and family are struggling.  They married early, but a lot of them ended up in divorce in part because of the economic stress they’ve faced (and the stress of trying to raise a kid with the first woman/man they ever dated, before they knew anything about what they might want from a long term relationship).  In a movie about a highschool reunion, these are the prom queens and prom kings who never left the small town and whose best days of their lives are behind them.  I’ve been to both meth and heroin interventions for people I care about in these communities.   I know people who abuse oxy because they are working extreme shifts (for not much money) at the only job available and they are taking pain killers just to keep going (compare to the lawyers I know taking speed to keep up with the hours, only without the nice financial compensation).  
>
>Now, my friends from college, and one of my liberal friends form highschool, and myself, have all been fairly successful.  We mostly spent our 20s “figuring it out” in one way or another.  I did grad school,  a friend of mine traveled the country working odd jobs and rock climbing, another friend bounced around a series of low paying jobs while trying to sell his art.  But we also learned how to sell ourselves, a bit of creativity,etc.  None of us are working the jobs we necessarily wanted, but almost all of us are now economically successful.  We have loans to pay off, which has delayed home ownership, and in some cases delayed children.   
>
>Most of us married late, some of us are unmarried but are effectively common law married.  I know for a fact that a more casual attitude toward sex allowed me to delay marriage long enough to get the education required for a good job.  All of us are in stable, loving relationships.  My relationship, because of some health problems and because of a period of unemployment, has weathered the sort of stress that destroyed some of my more conservative friends marriages.  We survived because by the time we got married, we both had dated enough people to know what would make a long term relationship work for us.  
>
>A lot of us now work far too much.  We’ve tasted some success in careers and have buckled down into it.  Some of us have had trouble having kids because we waited too long and are spending money on expensive IVF treatments.  But our kids and families are supported, etc.  
>
>Now, I don’t want to say that ALL my liberal friends are doing better than ALL my conservative friends, but it’s a distinct trend in my life that I see consistently.  And there is data supporting this idea.  
>
>Now someone like Charles Murray sees the statistics and says “man, these liberals are preaching that marriage isn’t that important, and that divorce is ok, and that you can be wishy washy,etc.  But look, they ARE in stable relationships!”  But I’m saying that my liberal values seem to have better prepared me for modern life than my friends’ and families’ conservative values did for them.  Conservative community and values worked well in the past, but the world changes really, really fast now.  A focus on flexibility, adaptability, diversity,etc might just be necessary to succeed these days.  
>
>So when people tell me the path to prosperity is to reinstill more conservative values, I think of the people I know, and from my perspective that just doesn’t look like it could possibly work. 
>
>Metaphorically, I know Fishtowners and now live in Belmont (my bubble is thin, to use Murray’s phrasing).  I was able to avoid early marriage and kids,etc entirely because I was very liberal about things like premarital sex.  It has also lead to (I think) more stable relationships because I was able to figure out what I really wanted over time.  
>
>My friend’s more conservative values had them marrying the first girls they wanted to have sex with, they had children early, and truncated their college experience.  They are now struggling to find work, and most have a divorce under their belts (the stress of low incomes + being married to people who it turns out they don’t have much in common with).
>
>Murray interprets this as Belmont liberals preaching one thing and doing another (I claim marriage isn’t important but I’ve been in a monogamous relationship for a decade), I obviously see it as Fishtown’s attempt to value marriage is counter-intuitively leading to bad outcomes.  

# The "benefit" of reading fiction

>So I think one benefit of fiction is learning to understand how stories fit together, how they are told,etc.  It doesn’t tell you a lot about reality, or judgement but it helps you understand how people talk about reality, how people think about judgement,etc.  
>
>So read Kahneman to learn about judgement (i.e. if you care about judgement, study judgement), but read fiction to learn about stories.  

# Szilard Engines, Information and Thermodynamics

>You’ll need a tiny bit of highschool level thermodynamics (relationship between heat and entropy, ideal gas law) to understand this point. 
>
>The goal is show that thermodynamics is intimately related to the theory of information.  
>
>There is a vast tradition in thermodynamics of considering different versions of Maxwell’s demon.  The idea is that we can better understand thermodynamics by working hard to design heat engines that try and extract work by decreasing entropy,etc.  We can then learn something interesting by the way in which the device fails.  
>
>To that end, consider this device, a Szilard engine .  
>
>What we have is a box with one gas molecule in it, plungers on either side, and a removable partition in the center.   The box is surrounded by an environment at some temperature T (your living room or something). 
>
>The way the device works is pretty simple.  
>
>1. We start by dropping a partition into the box (as shown).  
>
>2. Then we measure which side the particle is on (we could, for instance, weigh the box very carefully).  
>
>3. Now we depress the plunger on the empty side (this takes no work since there are no particles in that half of the box).  
>
>4. Now we remove the partition and let the gas particle do isothermal work on the plunger.  Volume is expanding, so the work done is W=∫pdV=Tln2.  This means that an equivalent volume of heat Q=∫TdS =Tln2 flows into the engine.  So the entropy of the outside system is lowered by S=ln2.   
>
>5. Drop the partition again, and the engine is back to where it started. 
>
>So what does this engine do?  It lowers the entropy of the environment in order to do work!  If this works as advertised we could lower the entropy in the environment down to 0, violating the second law!  
>
>So what is wrong with this engine?  Why would it fail? As Landauer originally pointed out, and (Shannon fully formalized) information carries entropy.   
>
>When we drop the partition down, before we can depress one of the plungers we have to record the signal of either right or left plunger (exactly 1 bit of information).  
>
>That hard drive where we record the data is effectively acting like the hot reservoir for our engine!  Our engine actually takes entropy out of the environment, deposits it into a hard drive, and does useful work in the process. 
>
>Information entropy isn’t analogous to thermodynamic entropy it really is thermodynamic entropy.  It’s this equivalence that sits behind Ed Jaynes’s interpretation of thermodynamics.  

# Renormalization group and deep learning

>So what is “deep learning?”  It’s showing up all over the tech news with google’s hallucinating neural networks, etc, but what exactly is it?  
>
>Let’s start with a simple machine learning model that every physicist currently working in machine learning that I know accidentally recreated, the hopfield network/boltzmann machine, although I’ll always think of it as the Ising machine learning model.  
>
>Let’s say you have a collection of black and white images,and you want to learn the patterns in the image.  Each pixel is either black or white.  So as a physicist you say “well, each pixel is either black or white, so maybe I should think of this as a spin, either up for down,” and you make a cluster of spins, like the crappy picture below. 
>
>So that is 4 pixels represented as spins.  
>
>Now, we can write down an energy (the Hamiltonian) for the spins (call them 1,2,3,4)
>
>`\[H = spin_1*spin_2*J_{12} + spin_1*spin_3*J_{13}+…\]`
>
>Here J are interactions between the spins.  
>
>So what computer scientists do to represent that , is they draw lines connecting each spin that interacts, so `J_12` is a line between spins 1 and 2.  They also call spins “neurons” because they think that the brain is sounds more interesting than spins (”spin networks” sound less interesting and hence will get less funding than artificial neural networks).  
>
>Now, we know in physics/thermodynamics that a system goes to a state that minimizes the free energy of the system.  So the goal of the learning problem is to find the interaction energies (J), so that the patterns you are feeding it are all free energy minima.   In a sense you are saying “these are the equilibrium states, what are the laws governing them?”  
>
>(Technical aside, we simulate exactly like you’d expect from thermodynamics, the probability of a spin flip is ~ \(e^{-H/T} \) and the free energy is calculated from `\(T ln \sum_i e^{-H(v_i)/T} \)`
>
>This is very “shallow” learning- our only spins/neurons are related to the input data (only 1 “layer”), so what good is it?  
>
>1. Imagine after training, you have a bunch of a corrupted pictures.  The spin network knows the right laws, so you can feed the corrupt data in, and let the spins change according to the learned laws of “physics”-  the corrupt pixels will fill in to complete the pattern. 
>
>2. Imagine you need to generate new instances of the same patterns.  You can turn the simulation temperature up, and let your system wander away from the low temperature equilibrium, then slowly turn it down and let it settle into a new ground state.  
>
>Unfortunately, it turns out that these sorts of models generally suck.  The real break through in deep learning was Hinton’s idea of the restricted boltzmann machine.  
>
>On the left we have the old network, on the right the new idea.  We introduce new spins called “hidden” spins to compliment our original input, and we introduce interactions only between the input layer and the hidden layer (you can see why these things get called layers now, so ‘deep’ is lots of layers). 
>
>But now what is the physical interpretation?  What are these hidden units? 
>
>  What we have is a new function that connects these layers (call the visible layer v and the hidden layer h)
>
>`\[C(v_i,h_i)\] `
>
>But look at what those hidden layers are doing- they average over the input layer with different weights.  The hidden layer is just like the block spins of the first post, and the function C is what connects the original microphysics to the new coarse grained/renormalized free energy.  
>
>(Technical aside, you can now write the partition function in terms of C, and then integrate out the visible units to get a free energy that only depends on the hidden units.  The trained model then minimizes this free energy of the hidden units).  
>
>And now, our ground states are all in terms of the hidden layer.  
>
>So the initial features are pixels, what sort of features might our new layer have in them?  Here is a picture of the hidden layer features/filters that show up when training on hand written digits (taken from this tutorial).    So the initial features are pixels, the next layer up we have various swoops and lines.  
>
>But we don’t have to stop there! We can add a new layer of hidden units.  After we train the first layer, we can then slap a new layer on and train it independently (essentially doing another renormalization transformation) 
>
>This introduces a new function 
>
>`\[C’(h_i,h_i’)\]` 
>
>And we’ll get even higher level features. 
>
>So deep learning is a process where we iteratively learn both the scaling transformation and the renormalized free energy (they are related to the same function, that we are learning C) from the data.  Each layer is at a progressively larger scale.  So in a facial recognition network you might see features emerge as pixels -> lines/curves -> eyes/chins/mouths
>
>For people who wanted a technical version of the blog post I’m noodling over, here is the paper that first >pointed out the connection.  It’s fairly straightforward: 
>
>http://arxiv.org/pdf/1410.3831v1.pdf
>
>So one of the interesting things that sort of falls out of a lot of modern physics is that the microphysics doesn’t really matter.  You could take the microscale physics, and add all sorts of messy, ugly complications and interactions but as long as you keep similar symmetries the same macroscale picture emerges.  So one reason the laws we know might look simple is that we’ve only really explored on fairly large length scales.  A theory at the planck scale could be disgustingly ugly, but probing at LHC type energies would give you nice simple laws because all the complications just don’t matter for long length scales.  
>
>So it’s not that you can’t reduce the high level behavior to something lower level, it’s that there are tons and tons of microphysical models that all lead to the same macro physics.  So for macroscale predictions it doesn’t matter what the actual micro scale laws are.  
>
>There are things where reductionism doesn’t tell the whole story- collective behaviors matter.  The whole point of a large thread of modern physics (renormalization group stuff,etc) is that a lot of macroscale physics is independent of the microscale physics and so you are best off studying the macrophysics itself.  
>
>Of course with these sorts of emergent behaviors you can still take the system down to quarks, but because the micro physics renormalizes out at larger scales, that isn’t going to tell you about most of what you care about.  You aren’t going to figure out symmetry breaking, etc, by looking at just the quarks or what have you.  

# The stockholder view

>I just had a really long conversation with an older gentleman who ran a fortune 1000 company several decades ago.  
>
>He had a really interesting take on inequality- he said back when he was running things, it was common to think of multiple stakeholders in a company- the employees, the community/costumers, the stockholders.  With the rise of what he called “Jack Welch bullshit” the focus shifted to only the considering the stockholders, with negative consequences for employees. 
>
>He pointed specifically to companies that goosed the stock price by manipulating pension plans ([something I already knew about from this book](http://www.amazon.com/Retirement-Heist-Companies-Plunder-American/dp/B00AK3WCZ8)) explicitly breaking trust with workers for the benefit of stockholders.  He also mentioned outsourcing healthy plants to other countries, not because managers actually expected to save money (”everyone knows the problems of communication with an overseas plant costs more than you save in overhead”) but because “the market loves when you cut the fat, and globalize, get people talking and the price goes up.”  
>
>I’ll add my own spin- I think the rise of the stockholder view is actually the rise of quantitative management.  The idea of creating quantitative metrics to judge performance on (which really appeals to my sort of people!) has serious problems when the only numbers you can measure don’t quite capture what you want.  You want to incentivize “running the company well” but the only number you can find is the stock price, so thats what people end up aiming at.  I tie this to the rise of the MBA. 

# On Bayesianism

>Bayesianism is a recipe.  It says “take your prior, collect data and do updates to the prior, that is your inference.”  It doesn’t tell you anything about the correctness of your inference,etc.  It tells you how to make inferences.  I’m saying that Bayes doesn’t make the best inferences in many situations.  
>
>What I’m suggesting is split your data into two sets, use whatever inference method you like, and then use the remaining data to ACTUALLY CHECK THE QUALITY OF YOUR INFERENCES.   Further- how you measure the quality of your inferences depends on what you are using them for.  
>
>I feel like you have this abstract magical thing you call “Bayes” and you just keep asserting that it’s perfect without ever using it to try to solve real world inference problems.  And if your “perfect” method cannot solve real world inference problems then it is not a perfect method, it’s a useless method. 
>
>I’ve found that when I have a large parameter space (like a lot of big-data type problems), then choosing a prior is a nightmare.  
>
>So called “uninformative” priors often aren’t- changing the parameterization can show you that what you think is uninformative is actually carrying a lot of assumptions.  You really need to construct a useful, realistic prior and it’s a fine line.  Too strong a prior and it overwhelms the data.  
>
>To make sure I’m avoiding this, even when I use a bayesian method, I mix in a lot of frequentist type checks on the model, etc.  I also usually “cheat” and use some empirical Bayes type stuff (estimating the prior from the data,etc).  I’m a big believer in “whatever is convenient for the problem I’m working on.”  
>
>Also, the other big Bayes issue is that the monte carlo methods for the integrals can be quite computationally costly.  
>
>Simulating QFT or GR does not require a hypercomputer.  It may require a non-deterministic turing machine to be simulated quickly, but everything definitely is computable.  You can still do it on a Turing machine. 
>
>The ideal Bayesian agent you describe, with a prior over every possible concern using Solomonoff induction is literally uncomputable.  You simply cannot do what you want with a Turing machine.  It’s not “possible in theory, intractable/impossible in practice” it’s impossible even in theory.  
>
>Now, I want to turn to a major difference between the physics you are citing and Bayes.  I know how to make approximations to GR to get classical mechanics- it’s a well defined limiting procedure.  I’m making calculations that really are approximations to the underlying theory.  
>
>However (to pick an example), when people in the real world do problems with sensor-fusion they usually find Bayes isn’t good enough.  So instead they go to a generalization of Bayes.  This isn’t an approximation to Bayes at all- it’s extending Bayes, it’s doing things Bayes can’t.  
>
>When people run into inference problems where Bayes isn’t tractable they generally don’t approximate Bayes, they generally use very different methods of inference.  
>
>In physics we say “hmmm, this equation is intractable. Let’s work in a long wavelength limit and see if the equation gets any simpler.”  In inference, much of the time it is “Bayes is a bad approach here.  Let’s extend it” or “Let’s chuck it out and use something else.” 
>
>And finally- if Bayes is often intractable in the very clean, well defined problems I work with every day as a data scientist, why should I expect it to work any better in situations where it seems absolutely impossible to define a useful prior?  
>
> > But consider physicists. They monomaniacally insist on following the laws of physics, even when they conflict with empirical results (like that the new reactionless drives work great!) or our intuitions (like our intuition that time doesn’t dilate when you go faster). Why can’t those physicists just accept that there’s a messy reality, there are no simple answers, and sometimes you should work with things like conservation of momentum but you’ve got to abandon them and use other tools when people ask nicely?
> 
> I think the difference here is that there aren’t “laws of induction” the same way there are “laws of physics.”  
> 
> Mathematical laws describe probability distributions, and how to manipulate them, but the problem of inferring how any given empirical data relate to an underlying probability distribution requires a lot of assumptions.  Most statisticians (correctly, in my view), are of the opinion that you should make whatever assumptions make a given problem more tractable.  
> 
> Rationalists often say things like “Bayesian is right and fundamental and frequentism is ad-hoc,etc.”  They also seem to think that frequentism is an approximation to Bayes.  But that isn’t really true- both are “right” given the assumptions they make about how to relate data to an underlying probability distribution.  
> 
> > Naive, unsuspecting frequentism can’t figure out the pharmaceutical company motivated-stopping problem you mention. If you just take the pharma company’s data and do a t-test on it, it’ll come out significant and look like the drug works. Indeed, this is the whole reason pharma companies do this in real life; the success of this ploy certainly can’t be attributed to all doctors being Bayesians!
> 
> The point here is that if you tell the frequentist about the stopping rule they will make the correct inference.  If you tell the Bayesian about the stopping rule they will not.  Bayesian statistics does not take stopping rules into account.  
> 
> You can see this in Yudkowsky’s own writing   Yudkowsky points to this situation as a feature of Bayesian stats, and the OP points to it as a bug. (Note, I believe he is confused when he talks of Bayes statistics as “laws.”  Bayes theorem is a theorem regarding probability distributions (that is a law), Bayesian inference is a set of assumptions about how to relate data to a probability distribution(not a law)).   
> 
> Of course, figuring out the exact experimental conditions for an arbitrary bit of frequentist inference is really hard.  But so is picking a prior.  
>
>I assume the machine learning and “frequentism” you have learned was in the context of applications and applications are always very messy.  All theory starts to look a bit hacky when you have to make the compromises required to work on real problems.  I assure you, in real world data analysis problems Bayesian methods are just as ugly (using the same data you are going to update with to condition your prior, choosing prior for a tractable update process instead of priors that fit reality,etc).  
>
>You are comparing a computationally intractable version of Bayesianism (but hey, it has a nice aesthetic), to a version of “frequentism” that has been shaped by the pressure of the real-world.  



# Could a super AI figure out relativity?

>Sure, but they would have to do it the way we do- by making very precise instruments for very precise measurements.  If the second order effect is too tiny to measure with your instrument, you’ll only find the first order effect.  
>
>Consider, for instance, special relativity.  One of the classic tests is the Hafele-Keating test, in which they put atomic clocks on commercial jets and flew them around the world.  They then compared them to stationary clocks to look at the time differences the travel caused.  So a flight around the world is probably something like ~100 hours of elapsed time, maybe more depending on layovers.  The difference between the clocks that SR predicts is ~100 nano seconds.  So to even notice SR in this instance you need a clock that can keep time to an accuracy of 10s of nanoseconds over several days.  
>
>Another example- Yudkowsky once claimed
>
> >A Bayesian superintelligence, hooked up to a webcam, would invent General Relativity as a hypothesis—perhaps not the dominant hypothesis, compared to Newtonian mechanics, but still a hypothesis under direct consideration—by the time it had seen the third frame of a falling apple.
>
>So lets ask a different question- is it even possible to detect Newtonian gravity from so little data?  We know the acceleration due to gravity is close to an inverse square C/r^2.  But we are dealing with the earth, so call the radius of the earth `R_e`, and d the distance the apple is going to travel in 3 frames.  
>
>`a = C/(R_e + d)^2`
>
>Now, d is maybe a milimeter or so- much smaller than the radius of the earth, so we Taylor expand
>
>`a = C/R_e^2(1-2d/R_e).  `
>
>so that important `d/R_e` first order effect is maybe ~10^-8.  This is already right around the wavelength of visible light.  No way to even see it with a webcam.  The best the AI can do is constant acceleration.  
>
>Basically, to get better laws of physics you need better data, which requires building better instruments.  It’s a bootstrap process.  

# IQ stuff

>So first, a correlation of 0.20 or so would be considered weak by most people.  
>
>Next, the issue here is that in social science, correlation are generally pretty weak.  So people will say things like “IQ is the best predictor of `_`”, but that’s only because there aren’t any good predictors (also, it’s rarely true that it’s the best predictor- i.e. the best predictor of criminality is probably previous criminality.  IQ is just a weakish, easily available predictor). 
>
>The rule of thumb I tend to use is that college board estimates the coefficient of SAT with first year college GPA is about 0.3.  Other IQ/stuff correlations are generally smaller.  So definitely pretty weak. 
>
>Edit: there are lots of ways you can get a big difference in averages and a low r.  This is why plots are always better than summary data.  
>
> >Isn’t Raven’s Matrices more heavily g-loaded than backwards digit span? Doesn’t that mean that if one is going one direction, and the other the other direction, we should be more likely to trust Ravens as what’s really happening to g, and assume backwards digit is about some other mental faculty that’s idiosyncratically declining while g is going up? Is there any justification for the authors making the opposite assumption?
>
>Certainly in the Weschler bank, both forward and backward digit span have pretty low g - loadings, though forward is about half that of backward.  
>
>But if you trust Raven matrices as to what’s really happening to g, you end up with huge gains (the raven tests have, as of late 2000s when I last cared to look at such things the highest Flynn effect gains), and the awkward prediction that our grandparents and great grandparents probably couldn’t grasp the game of baseball.  
>
>So people who want to rescue the construct attack the idea that Raven Matrices are g correlated (maybe it’s just the modern age training people to be better at abstract tests) They also attack tasks on the performance side of Weschler (think picture arrangement, picture completion, block design,object assembly, etc) because they have equally high gains.  
>
>So after kicking out the tests that look like “pure” test of g but have high gains (unlike less “pure” vocabulary or information which have nice g loadings but clearly improve with schooling), you are left with stuff like digit recognition.
>
>First, most of the reason that the gains from Raven are on the “least g-loaded” items is in part because after the gains from Flynn effect, the variance (and hence covariance) on the tests where people improved the most has gone down.  So part of the “these items aren’t g-loaded” is that after people have improved those tests don’t look g-loaded anymore for the precise reason that because everyone has improved so much, these tests no longer predict differences. 
>
>Like, look at the WISC tests we have data for: 
>
>Arithmetic  
>
>Block Design
>
>Coding
>
>Comprehension
>
>Information
>
>Object Assembly 
>
>Picture Arrangement
>
>Picture Completion
>
>Similarities
>
>Vocabulary
>
>Now, at first glance we might say arithmetic, vocabulary and information might be the least g-loaded tests, and that object assembly, coding might be highly g-loaded because they are so similar to Raven matrices.  And with early samples, this is more or less true. 
>
>Now, look at the Flynn effect gains (listed least to largest, from Flynn’s 2007 book): 
>
>Information+2 
>
>Arithmetic+2 
>
>Vocabulary+4
>
>Comprehension+11
>
>Picture Completion+12
>
>Block Design+16
>
>Object Assembly+17
>
>Coding+18
>
>Picture Arrangement+22
>
>Similarities+24
>
>So you couldn’t have predicted ahead of time that the Flynn effect would be on the “least g loaded.”  Rather, they are on the tests that Raven and others considered the most g-loaded.  If you were to do a confirmatory factor analysis with factors from a 1950s test with tests from a modern cohort, it would fail robustly.  
>
>So all that I’m saying is when people say “neural conduction speeds correlate with g” it’s important to specify when that was done.  If the Flynn effect is a test familiarity/modernization issue (as Flynn himself has always suggested) then an IQ test cohort consists of people familiar and people not familiar.  This is a recipe for spurious correlations because there is an unobserved variable (test-familiarity) that has a huge effects on scores.   
>
>Imagine a world where (for some bizarre reason, everyone is height blind or something) the only way we have to measure height is by the number of points someone scores in a game of basketball.  
>
>No one has ever said intelligence isn’t important, what people are saying is that the way we are measuring intelligence is so flawed as to be nearly useless. 
>
>I’ll say this regarding the statistical argument against g - if someone at work told me they had lumped all of the different predictors of a bad claim into a single number, and they called it a “severity quotient” or something and it represented something causal about the claim,  I’d tell them they were being very silly.  But I don’t think this will be convincing to people who don’t do much with statistics.  
>
>So I’ll steal a point from Flynn (these next two paragraphs are a point I think I originally read in Flynn, but it has been awhile). 
>
>If you look over old scores (sort of looking at the Flynn effect in reverse), older generations had remarkably low scores.  It will help here to have a qualitative idea about what these things mean- Arthur Jensen in “Straight Talk about Mental Tests” describes someone with a score in the mid 70s as someone who, even if they really tried to follow baseball would be vague about the rules, unable to name teams other than the home team, unsure of how many players should be on the field,etc.  
>
>So with just IQ historical data, we have to conclude that around WWI (about as far back as we have actual data for) fully half of white Americans couldn’t understand the rules of baseball (keep in mind it was dubbed the national pastime in the 1850s or so, when presumably even less of the population understood the rules).  You can play these same games with other sports and other countries.  You can play these same games by looking at the IQ estimates of countries today, some of them are just ludicrous.  The alternative is that IQ tests have a shifting relationship with underlying notions of intelligence. 
>
>I want to take a step back- we seem to agree, primarily- the Flynn effect is NOT in fact increasing intelligence in the sense of innate ability (as I said, it would be absurd to conclude that half of Americans couldn’t understand the rules of baseball around WW1).   We also, I hope, agree on the empirical evidence that Raven matrices do show a strong Flynn effect, despite having a g loading of something like ~0.80 which is quite high.  
>
>The links you are showing me are arguing that the Flynn effect gains are “hollow” in that they don’t correspond to innate ability, which I partially agree with.  I think they don’t correspond to innate ability, I don’t buy all the implications of the word “hollow.” 
>
>What I’m suggesting this means is that the relationship between IQ tests and actual innate ability changes over time in non-trivial ways, and may change across cultures in non-trivial ways.  If I give the same test to an ancestor and take it myself, it’s not “correct” to directly compare scores.  
>
>I think the IQ/g proponents realize this, which is why they start jumping through more silly statistical constructs (splitting g into fluid and crystallized g,etc)  Given that I don’t think that g is a particularly interesting statistical construct, I obviously think adding these are just epicycles.  
>
>I think Shalizi’s objection covers a lot of what I’d say- basically reification of factor analysis is a mistake.  It’s a description of the data, not a model.  Causal discovery is a hard problem!  
>
>The big claim of IQ is that it- makes-sense-to/you-can represent human intelligence as one single number.  
>
>The “IQ is a statistical myth” critique, or at least the critiques along the lines of Shalizi, Nostalgebraist, or myself is that the methodology used by psychometricians is incapable of distinguishing between hundreds of intelligences or one type of intelligence.    
>
>If what you care about is how do people actually think/what IS intelligence IQ is basically worthless.  
>
>If instead what you want to do is create a fairly weak (R ~ 0.2 ish) predictor of various things, then IQ is fine.  This is telling you that In general people who are better at lots of cognitive stuff averaged together also have a slightly better than 50% chance of being better at their jobs.  What does that actually tell you? How important is that claim, really?   
>
>You can create similar metric by combining height, shoe-size, weight and waist circumference (these will all be positively correlated, so you can guarantee a largeish first factor), and call it a ‘sports index’ or something.  (It will be correlated with wages to roughly the same extent as IQ, given teh well known height/IQ correlations with income), and it will probably correlate with performance in various sports.  But if you were to notice that all basketball players had similar 'sports index’ to all football players, you are missing something important about the actual build of the various players.  
>
>I’ve been reading some of the IQ discussions happening here for the last few days, and occasionally yell at the computer or punch some thoughts into some bloggers ‘ask’ box.  After days of this, I decided to weigh in myself.  
>
>So, lets dig in- first, is IQ “real” or a “statistical illusion.”  This depends entirely on what you mean by real.  So, claims:
>
>Claim 1- lots of different academic test results are positively correlated.  In a large collection of tests, people who do well on 2 or 3 tests usually do well on all of them.  
>
>This is absolutely true. We’ll call it “the positive manifold” from here on, because thats what the literature calls it. 
>
>Claim 2- this means that we can do a factor analysis and pull a single large common factors and call it “g” or “general intelligence” or “IQ” or whatever. 
>
>Sure, go for it. 
>
>Claim 3- the best model to explain this is that there is a 'general intelligence’.
>
>False.  There are all sorts of underlying models that can explain these correlations.  You could dozens or hundreds of specific intelligence which are moderated by a single “test taking” factor.  
>
>You could also make a model with hundreds of specific intelligences, where tests randomly test for just a few of them.  Cosma Shalizi discusses this in more detail than I really want to.  
>
>The large point here is that the positive manifold is simply not enough empirical evidence to separate a multiple intelligences model from a 'general intelligence’ model.  They all create the positive manifold.  
>
>Because the concept can’t separate competing models of intelligence, but construction, IQ is of limited scientific use if you want to know “what is intelligence,” and get at details of the underlying phenomena. 
>
>(A semi-related note- on “test taking” as a realistic factor.  I do horribly on what-is-the-next-number-in-the-sequence type questions i.e. 1,1,2,3,5.. what comes next?, usually because I can construct plausible rules for lots of different answers.  My brother is a smart guy who gets terrible scores on Raven’s matrices for similar reasons. I include in test-taking ability being able to answer “which answer was the test writer thinking of?”)
>
>>So maybe IQ doesn’t measure a single general intelligence, but we are averaging tests to get the score, so even if its just a weird aggregate, it might contain some useful information.  Maybe we can’t use it to figure out whether intelligence is “general” or lots of specifics, but it is an easy number to measure, so we can go correlation hunting and use IQ to explain the world around us.  
>
>Maybe.  You have to remember that in social sciences, effects are small.  I could go through a laundry list of potential correlations, but lets look at one thats likely to be the strongest- using the SAT to predict college grades. 
>
>The people who publish the SAT  suggest that the unadjusted correlation between SAT score and GPA is pretty weak (using the old school rule of thumb for Pearson’s R where 0.2 is the cut off for weak, 0.3 for moderate, 0.4 for strong, 0.7 for very strong).   
>
>For student’s admitted to colleges, SAT score is a weakish (its on the boarder of moderate) predictor of college GPA.  It covers a bit less thn 9% of the variance
>
>The college board also does some adjustments to control for the fact that only a narrow range of SAT scores get admitted to colleges.  These push us up to a 'strong’ correlation, but that’s not directly applicable to predicting GPA from SAT score.  Instead, it tells us  
>
>SAT is a good predictor of whether or not a student gets admitted to a college. 
>
>I hope this latter fact surprises no one.  So, according to the most pro-test source available, SAT is a weak-to-moderate predictor of college GPA.  Of course, the GPA/SAT thing is looking at one specific point in time.  
>
>So IQ (or at least the SAT proxy) is an ok predictor, but not great.  I’’m having trouble finding good data on after freshman year.  Nearly everyone agrees this correlation gets smaller after freshman year, but no one agrees how much. 
>
> >What about using IQ to compare whole countries/industrialization? 
>
>This is largely a joke.  The data is very spotty, so researchers fill in and supplement from other tests, often making adjustments without commenting or dropping whole test batteries from the sample.  The result is confirmation bias run rampant.  
>
>To give an example via anecdote- my cousin has down’s syndrome but is very high functioning (for someone with down’s syndrome).  His estimated IQ is 70 or so.  
>
>Many years ago, I traveled all over Mozambique, a country that the psychometric literature suggests has an IQ of roughly 70.  The literature suggests ~half of the people I met in Mozambique should have a similar IQ to my cousin.  I did not find this to be the case.   


# Disagreements with Less Wrong

>My critique is actually that the type of people being drawn into the movement by these conversations are likely to enjoy talking about ideas instead of following through on ideas (I think this is kind of a problem in the lw-sphere in general.  Lots of people talk about Bayes or whatever but hardly anyone can do an actual Bayesian analysis.  Lots of people talk about AI risk, hardly anyone reads up on what is going on in the field,etc).  
>
>At the end of the day, if you have tons of people debating the most effective way to give, but no one actually giving money then the movement is failing at its goals.  
>
> >Would you say that a common failure mode of Less Wrong is overemphasising the “theoretically correct” way to do things and looking down on approximations that are much more useful?
>
>I would go a step further and say that LW paints the approximate/useful methods as insane in order to make a case for a specific “theoretically correct” way.  
>
>I’m getting asks left and right asking me to dive in on the various LW arguments that are happening on my dash.  I might do that, but I wanted to talk a bit about “practical” Newcomb problems.  Unfortunately, my brain has been puzzling out some reactions to some of those LW discussions, as well as the Newcomb problem stuff, so everything is all muddled in my head.  So there might not be a great point in what follows.  
>
>A lot of Yudkowsky’s writing takes the form 1. most of the uninitiated rubes think this 2. Here is a clear argument/what the rationalists should think (generally, this argument is only clear because it has skipped over many of the common counterarguments) 3. Why do the rubes think this? Conformity/THE WORLD IS INSANE.  
>
>I find this tedious because usually I find myself agreeing with the rubes for reasons that aren’t presented, AND because this same attitude spills over to many of the LW people I’ve interacted with.  At first,  was sort of excited about the LW influence on some of my friends, because I love physics- and they were learning quantum mechanics! OH MAN! THEY’LL HAVE SO MANY QUESTIONS! But then it turned out they only cared about quantum mechanics in so far as they have the secret knowledge of the true interpretation.  So I worry a lot (you’ll see this in my other writing on LW) that LW imparts a sort of curiosity stopping “this is the right way/everyone else is just crazy” sort of mentality.  
>
>Now, to pivot to decision theory (while coming back to the above point in just a second) I firmly believe causal decision theory is strictly better than evidential decision theory (EDT). But, I make systems to aid in decision making professionally, and every one of those decision/prediction systems I’ve built is basically doing something like EDT under the hood. Further, I can only think of one instance of causal analysis in all the systems I’ve ever even seen implemented.  
>
>So Yudkowsky could write another one of his posts (hell, this post might exist for all I know) where 1. most of the rubes use evidential decision theory in their analytics 2. causal decision theory is obviously better 3. THE WORLD IS INSANE. And I would agree with 1 and 2, but 3 doesn’t follow.  
>
>Why do I use EDT? Is it because I’m insane? No, it’s because it’s tractable (it’s down right easy), and causal discovery is really hard. With the exception of well controlled scientific experiments, it’s pretty close to impossible. That is why social science is hard, and why the scientific method is what it is. I can’t emphasize enough how hard causal discovery is.  
>
>These are the sort of practical objections that get glossed over. I’ve had students in science classes ask why real scientists don’t just do Solomonoff induction- it’s literally uncomputable.  
>
>Some of these EDT type models I make are very good.  In some cases I hit predictive accuracy in the 80+% range.  So now, if I go to an insurance company with a list of claims and say “80% of the claims on this list will cost $100,000 or more” (to pick an arbitrary dollar amount).  
>
>Now, the insurance company has a choice- it can go with my prediction, hold more money in reserve for the claim, and move on with their lives (sort of like Newcomb one box, very loosely).  Or, it can fight the prediction.  It can, for instance, try to settle the claims- maybe they go to a claimant who has only spent $5k so far, and say to them “hey, we’ll give you a lump sum of $50k! Doesn’t that sound great?” (sort of like Newcomb two box, loosely).  
>
>In reality, this means that my predictions break- I often go to a company with a model that works 80% on historical data but as soon as they start receiving predictions the settlement rate goes way up, and now my model is only 15-20% accurate!   
>
>So anyway- to sum up my points about non-idealized “real world” Newcomb:
>
>1. In the real world, making a prediction can influence the outcome- and in fact that is often the whole point of making the prediction!  There are no perfect predictors,  and timeless decision theory muddles this.  
>
>Hence, the real world is more like transparent Newcomb than opaque Newcomb- if someone sees both boxes filled should they really only 1 box?  From this perspective, a pre commitment “solution” to transparent Newcomb is sort of ridiculous.  
>
>2. The actual, practical implications of Newcomb depend on how you map the problem on to what you care about.  It’s supposed to get you thinking about how prediction might interact with decision theory.  “Solving” the toy thought experiment is meaningless- what matters is using insight gained from it regarding how prediction can impact decision theories.  And that insight varies from practical problem to practical problem.  
>
>CDT is in the sweet spot of obviously correct and sometimes applicable (you can design trials to make it applicable, and you should!) .  “TDT” feels like a hack (notice the 100+ page paper Yudkowsky wrote on it never comes out and formalizes the theory!) that is designed specifically for Newcomb-like perfect prediction problems that is applicable nowhere else.  Imagine trying to design situations in order to appropriately use TDT?  They all involve weird pre-commitments to certain stances, regardless of the expected value when that situation comes around! 
>
>And it’s just this sort of practical application that I think LW skips over- Bayesianism is obviously correct, until you try to use it to solve a problem.  Everyone should use solomonoff induction (but you can’t, because its uncomputable),etc. 

# Recommended reading instead of HPMOR

>Fresh in my mind because he passed away recently- Terry Pratchett.  
>
>Also, there is a lot of entertaining non-fiction- read Kahneman, Erik Larson, Mary Roach,etc. 

# On the "sequences"

>I don’t think they are concise, and I think that a lot of what makes them easy to read/understand is that they strip out the subtlety and the strong opposing arguments.  Intro texts tend to cover more than one viewpoint, which I think makes them more difficult.    
>
>I think another thing that makes the sequences easy to read in a bad way is that a lot of the examples are distorted to the point of just being wrong.  To use an example that I think was in the mysterious answers sequence, phlogiston.  Phlogiston was described by Yudkowsky as some sort of “mysterious” non-explanation of fire.  This is just wrong.  Phlogiston was a legitimate scientific theory- it tied together burning and rusting a full 100 years before the discovery of oxygen.  Phlogiston theory is also almost correct- phlogiston is basically what happens if you try to guess at the existence of oxygen, but “get the sign wrong”- it’s a sort of negative oxygen (subjects oxidize when they burn, and the air loses oxygen.  vs. the theory which says subjects lose phlogiston when they burn, and the air gains it).  When Priestly discovered oxygen, he thought of it as dephlogistonated air. Phlogiston also made concrete predictions- which is how it was falsified.
>
>I’d recommend Thinking Fast/Thinking Slow for the stuff on biases.  For the probability stuff, I’d recommend really any intro probability text.  
>
>I think more and more I’m convinced that people’s response to the LW sequences depends strongly on how much of those ideas they have encountered before.   
>
>I actually encountered the Singularity Institute pre-sequences, and by the time the sequences were showing up on Overcoming Bias I was already pretty far into grad school.  I’d already encountered Jaynes (who I read for his thermodynamics interpretation more than the probability stuff), I’d seen the Kahneman biases stuff in Intro Psychology classes, etc.  So the sequences felt like someone grabbing disparate ideas I was familiar with, stripping off some of the caveats that came along with those ideas, and then stitching them into a weird canvas
>
>I would say Yudkowsky is not a good resource for learning how to solve actual probability and statistics problems- his sequence posts are largely advocacy of a very specific philosophy of probability, without much in the way of the practical.  I imagine Khan academy, or coursera have intro probability courses that are more systematic.  
>
>But the thing that kicked off this round of talking-about-quantum is that I said offhand that Yudkowsky talks about “map/territory” type fallacies, but immediately jumps in to talking about configuration spaces and wavefunctions as if they were territory (without even mentioning this huge hidden assumption).  This struck me as ironic.  
>
>Also, the point of the quantum sequences is that it’s a tent pole in his “science vs bayes” discussion. Physicists not being rational enough to accept many worlds, and all that.  
>
>I think it’s a good lesson in the difficulty in escaping cognitive biases that someone who can spend so much time writing about, say, “map is not the territory” can write a whole sequence of posts about how the wavefunction of quantum mechanics is the territory, for instance.  
>
>Many years ago now, I took a survey course on philosophy that was actually very good.  The professor had a way of convincing us young freshman of the validity of almost any argument presented, at least at first, and so the experience of the course was a bit unnerving. You’d find yourself walking out utterly convinced of a position that would be thoroughly demolished the very next class.  
>
>I bring this up only because this evening I had a long scotch-fueled discussion with a friend, who believes himself thoroughly Bayesian. However, never in his ‘Bayesian enlightenment’ has he encountered serious opposing viewpoints. I’m in no way an expert on such matter, but I have been working with machine learning for a few years now and so I have run into problems day to day where Bayes is just a terrible approach.  
>
>Also, Persi Diaconis is an intellectual hero of mine (he was a magician turned mathematician. I wanted to be a magician at one point, and ended up with a physics phd. Actually, now that I think about it, my real magic trick- after a phd in theoretical physics I convinced an insurance company to pay me to do statistics- TA-DA), but I digress. Diaconis and Freedman had a rather famous result about fairly well behaved situations where Bayes fails. Even worse, Bayes fails without telling you it has failed- it looks like it has converged. Yes such cases are over infinite spaces, but a lot of interesting questions involve such.  
>
>But my friend, a self-declared Bayesian had never heard of these issues. He was also somewhat surprised when I suggested that most statisticians are pragmatists who will readily switch back and forth between Bayesian methods and non-Bayesian depending on the problem. There is no big war, no sides are being drawn.  
>
>This got me thinking about how important it can be to have a guide to a new intellectual arena. If my philosophy professor had stopped after presenting only half an argument (like reading just the Either of Kierkegaard’s Either/Or), I would have left that class utterly convinced. Even worse, I could have been convinced of either side.    
>
>Big Yud, in his sequences, is clearly an advocate for lots of ideas, but I fear that he isn’t a great guide into that intellectual realm. As an autodidact, no one has ever forced him to read in depth about viewpoints (or anything really) that he isn’t interested in.  
>
>I think its because of this eclectic, unguided collection of knowledge that I used to occasionally run into undergrads in my quantum classes that KNEW many worlds was the right quantum mechanics interpretation, even though they couldn’t follow a simple scattering or decay example I tried to use to cast some doubt on the issue.  
>
>Its why I’ve encountered people on the internet telling me that all scientists should do Solomonoff Induction.  Apparently nobody told them its not computable, and even rather slowly asymptoting approximations are exponential (and therefore just about useless).
>
>I’ve even surprised sequence readers by suggesting most scientists don’t think Drexler style nano-tech is possible (i.e. Richard Smalley won that debate), presenting results of an informal poll of physics grad students in my old department.  

# On transhumanism

>I would consider all of the work people are doing today to develop machine learning, to develop efficient storage and retrieval, etc (hell, even Hutter’s work developing the exact Bayesian AI that the Less Wrong sequences paint as the perfect reasoner) is doing productive and important work that makes the world better tomorrow.  It’s only after we understand which ideas are working that we can begin to develop safety standards.  So when you say “preparing for 2100,″ I think most fields of research actively are, and the philosophical work coming out of these FHI type places are just amusing entertainment.  
>
>Similarly with cryonics- I think that there are biologists working on the promising avenues toward anti-aging, etc, but it’s still too premature to know where that is going to lead.  There are people studying the brain, there are people working to characterize brains by freezing, slicing and scanning them,etc.  All of this is the preliminary research that will lead us in the promising directions.  The people working on Open Worm might lead toward whole brain emulation,etc.  
>
>Trying to shortcut the basic research and skip to the applied research doesn’t work.  And I DO firmly believe we should be spending more money on basic research- (at least in part selfishly- I personally think my talents would have been better spent in physics research than in data science).  
>
>But I think the ground work hasn’t been done yet for most (maybe all?) of the common transhumanist plans.  This is not a new criticism- it’s similar to what Smalley said of nano-tech and Estep et al. leveled at SENS during the technology review thing.  
>
>The response to both of these things is to keep pushing the edges of science forward until the paths forward present themselves. 
>
>Research builds on research.  It always has.  So a new discovery will come from somewhere which is my point- maybe it’ll come from making machine learning algorithms that generalize better and better.  I’m already using algorithms designed for image analysis to analyze insurance claims- that is surprisingly general.  Maybe it will come from neuroscience research (someone emulates a whole brain,etc).  
>
>The people who actually work in these fields are pushing knowledge forward everyday.  The transhumanists (and MIRI) are mostly sitting on the sidelines and occasionally bitching that the methods are too ad-hoc.  
>
>Progress won’t come from someone who “solves AGI” literally from scratch.  Discovery builds on discovery, research requires extending actual concrete ideas.  
>
> >And finally, the fact that there’s a really huge gap between today’s tech and AGI just makes it even more imperative that we work as hard as possible. People are dying every second, you know!
>
>There is no real reason to suspect that creating AGI will “solve” death.  That is more magical thinking, defining an AGI as essentially all-powerful-god.  
>
>Barring that, what do you think “working on the problem” should look like?  It’s not like you sit down and say “I’m going to solve AGI” and think real hard for awhile and then write the answer down.    
>
>Working on the problem looks like refining and extending machine learning algorithms (with the added advantage that you make material improvements in the world today, while also making incremental progress), or working in neuro biology, etc.  Which doesn’t really describe, say, MIRI, whose primary accomplishment is the large dose of rationalization needed to convince themselves of their Design unimplementable-perfect-decision-theory -> ? -> all the world’s problems are solved plan.

# The sum of all natural numbers = -1/12

>I once had a giant drunken argument with a string theorist that went similarly. Like, I walked away from seeing that with the thought “man, we really need to be careful with analytical continuations” and apparently a lot of people walked away thinking “how awesome and counterintuitive" 

# Anonymous MIRI ask

> > I'm a long time member of the rationalist community. I'm glad MIRI exists, but I wanted to say that you are right about their effectiveness. It is actually much worse than you think. The rationalist community has a huge Dunning-Kruger problem. We explicitly believe that intelligence trumps domain knowledge (see sequence post "The Level Above Mine," or "Above Average AI Scientists."). 
> > 
> > Interview with MIRI or even CFAR and they’ll ask questions about your IMO score and how much of the sequences you’ve read. They won’t ask about your knowledge of machine learning or your experience in outreach/education. So we get all these bright people in the community, but the domain expertise is narrow. None of the researchers have been successful researchers before. 
> > 
> > Everyone is very earnest but they don’t even realize what a good research environment looks like. I didn’t until I left and began working on my phd. Yudkowsky is brilliant but has no patience for the grind of research so he works on a project until he gets bored with it. I had thought Luke was making important changes until MIRI posted its job advertisement for a science writer to write up technical results. That’s just not how formal research works. 
> > 
> > And the institute suffers from group think. Everyone hired is fully on board with the sequences, and is convinced that the path to AI is formal mathematical results. I was too. But since starting my phd I’ve discovered that a lot of the critics have a point. Most of the MIRI ideas around CS and AI are unimplementable. In retrospect the ideas are not as brilliant as I once thought. 
> > 
> > I showed the Flare programming language notes to some people in my program and they tore it apart. I tried to use a Bayesian method in a signal processing class and it was one of the worst performers in the class. And you were right in your debate with Scott, there really aren’t any interesting published papers. The one conference proceeding is just a trivial category theory result. MIRI isn’t doing CS research anymore. It does math that is interesting but not useful for AI. I’m worried.

# Map vs territory

[In response to Scott and Scurvy](http://www.idlewords.com/2010/03/scott_and_scurvy.htm):

>It’s a good story about the theory ladenness of observation.  Without the vitamin model of nutrition, it’s easy to use the same observations to reach the wrong conclusion (vitamin C in lemons prevents scurvy vs. acid in lemons helps kill harmful bacteria in preserved foods).  It’s also hard to make sense of the idea that storing the same juice in open air and piping it through copper kills it’s useful property.  
>
>For better or for worse, physics is limited in what it can tell you about the territory, it can just provide you with more accurate maps.  Often it provides you with multiple, equivalent maps for the same situation with no way to choose between them.  
>
>For instance, quarks (and gluons) have this weird property- they are well defined excitations at very high energies, but not at all well-defined at low energies, where bound states become fundamental excitations.  There is no such thing as a free-quark at low energy.  For some problems, the quark map is useful, for many, many more problems the meson/hadron (proton,neutron,kaon,etc) map is much more useful. The same theory at a different energy scale provides a radically different map (renormalization is a bitch, and a weak coupling becomes strong).  
>
>There is no “true math of quantum mechanics.”   In non-relativistic, textbook quantum mechanics, I can formulate one version of quantum mechanics on 3 space dimension 1 time dimension, and calculate things via path integrals.  I can also build a large configuration space (Hilbert space) with 3 space dimensions, and 3 momentum dimensions per particle, (and one overall time dimension) and calculate things via operators on that space.  These are different mathematical formulations, over different spaces, that are completely equivalent.  Neither map is more appropriate than the other.  Hariezer arbitrarily thinks of configuration space as the RIGHT one.  
>
>This isn’t unique to quantum mechanics, most theories have several radically different formulations.  Good old newtonian mechanics has a formulation on the exact same configuration space Hariezer is thinking of.  
>
>The big point here is that the same theory has different mathematical formulations.  We don’t know which is “the territory” we just have a bunch of different, but equivalent maps.  Each map has its own strong suits, and its not clear that any one of them is the best way to think about all problems.  Is quantum mechancis 3+1 dimensions (3 space, 1 time) or is it 6N+1 (3 space and 3 momentum + 1 time dimension)? Its both and neither (more appropriately, its just not a question that physics can answer for us).  

# Against secretive science

> >i hate every time a fictional scientist is like “I couldn’t use this technology responsibly, time to destroy it and protect others from making the mistakes I made”
> >
> >Like…. I feel like making a list of technology originally created for nefarious purposes that we’ve since put to better use
> >
> >I’m Victor Frankenstein, I used rockets to bomb London, I must destroy this technology so nobody else can work the horrors I worked with it. No, NASA hired a Nazi rocket scientist and he took us to the moon
> >
> >First computation on the first electronic computer? Part of hydrogen bomb research. Victor Frankenstein would probably destroy it after using it to bomb Japan and being haunted by the screams of burning children, and then we wouldn’t have Tumblr
> >
> >and that’s just technology. The really awful things about what Frankenstein and Jekyll did isn’t destroying their particular technologies, it’s closing off the study of a novel phenomenon. Like…. Dr. Jekyll’s formula for turning into an evil version of yourself is maybe not a particularly useful invention, but where does the mass go? Where does the evil version’s features come from, your imagination? A direct effect of your imagination on reality? The phenomenon being available to scientific study is far more important than the technology itself
> >
> >FOR EXAMPLE. MPTP is a chemical that induces Parkinson’s-like symptoms. It was discovered by people trying to make MPPP, an opiate, to get high and to sell it to heroin addicts. This resulted in a lot of heroin addicts getting irreversible brain damage and Parkinson’s like symptoms. At which point Viktor Frankenstein sees the evil worked by his greed and destroys the formula, right?
> >
> >But… how does it induce Parkinson’s-like symptoms? That was the question that scientists superior to Viktor Frankenstein asked. They started feeding it to monkeys to study how it does its damage, figure out whether similar but weaker toxins might be responsible for some cases of parkinsons, and figure out if drugs that block the damage from it could be used as parkinson’s treatments. source: http://content.time.com/time/magazine/article/0,9171,141542,00.html
> >
> >So, like…. a chemical to give people parkinson’s disease is even more useless than a formula to turn you into an evil version of yourself, or a way to create humans out of dead body parts. But what was important wasn’t the technology, it was studying how it worked, to improve our scientific understanding.
> >
> >That’s why I hate Dr. Jekyll and Viktor Frankenstein
>
>This is a large part of why I dislike even the science bits in HPMOR. Things like- the “Interdict of Merlin” which prevents powerful spells from being written down, in the latest chapter I read it turns out the rules for making potions aren’t written down or shared, and Hariezer starts his scientific investigations with Draco as a secretive conspiracy (to make sure the wrong people don’t get the knowledge ostensibly), all of these fit the Jekyll, Frankenstein, we-don’t-share-ideas-we-hide-them-because-they-are-dangerous-mold.    
>
>This “ideas are too dangerous to let out” goes against the culture of science and prevents others from building off your ideas, which is SO important to science.  

# Choice of programming language

>So there are two things to think about really, one is “how much work will I have to do to code this up?”  and “how efficient does the code need to be?”  
>
>For the first one, a lot of the modern high level languages (python,ruby,whatever) are great answers, but before you start coding you should see if there is already an existing community of users already working on your (or  a similar) problem.  In the case of data science, python has a large data science base, with tons of packages that are designed to make the common data manipulation and modeling tasks easy, Ruby doesn’t have that.  Python also has cython, which lets you compile static machine code, so you can get a speed improvement if you are doing something numerically intensive.  Basically, its not so much about the language as all the infrastructure around the language.  
>
>Similarly, if you need to do statistics, R is the language of choice because of how many people are already doing statistics in R.  For almost any common statistical problem, all you have to do is load your data, download and load the necessarily library, and call one or two functions.  

# Statistical methods vs understanding the problem from first principles

>I think a lot of problems that seem really hard when you think about trying to actually understand the problem become surprisingly straight forward when you instead just collect a bunch of data and do some statistical analysis. 
>
>For instance, with the advent of statistical methods, instead of trying to use expert heuristics and play out as many moves deep as possible, chess engines got WAY, WAY better.  In the late 90s, it took specialty hardware like Deep Blue to stand a chance against a grandmaster.  Now an open source engine like StockFish playing running on a 486 computer could probably beat Magnus Carlsen (best player in the world currently).  The downside to this approach is that there seems to be a limit to how much you can actually learn about the game of chess by doing this.  
>
>Speech recognition is similar. 
>
>The physicist in me struggles with this problem at work sometimes as well.  I have very sophisticated models that are very good at doing what I’m tasked to do, but I don’t feel like I really understand anything from a first-principles standpoint.  All description of the data, no real theory. 


# Simulation theory of empathy

> In order to answer last night’s science question, I spent today slaving on the streets, polling professionals for answers (i.e. I sent one email to an old college roommate who did a doctorate in experimental brain stuff).  This will basically be a guest post. 
> 
> Here is the response:
> 
> >The first thing you need to know, this is called “the simulation theory of empathy.”  Now that you have a magic google phrase, you can look up everything you’d want, or read on my (not so) young padawan. 
> >
> >You are correct that no one knows how empathy works, its too damn complicated, but what we can look at is motor control, and in motor control the smoking gun for simulation is mirror neurons.  Rizzolatti and collaborators discovered the certain neurons in macaque monkey’s inferior frontal gyrus that related to the motor-vocabulary activate not only when they do a gesture, but also when they see someone else doing that same gesture. So maybe, says Rizzolatti, the same neurons responsible for action are also responsible for understanding action (action-understanding).  This is not the only explanation, it could be a simple priming effect.  This would be big support for simulation explanations of understanding others. Unfortunately, its not the only explanation for action-understanding. There are other areas of the macaque brain (in particular the superior temporal sulcus) that aren’t involved in action, but do appear to have some role in action[understanding. 
> >
> >It is not an understatement to say that this discoveryy of mirror neurons caused the entire field to lose their collective shit.  For some reason, motor explanations for brain phenomena are incredibly appealing to large portions of the field, and always have been.  James Woods (the old dead behaviorist, not the awesome actor) had a theory that thought itself was related to the motor-neurons that control speech.  Its just something that the entire field is primed to lose their shit over.  Some philosophers of the mind made all sorts of sweeping pronouncements (“mirror-neurons are responsible for the great leap forward in human evolution”, pretty sure thats a direct quote)
> >
> >The problem is that the gold standard for monkey tests is to see what a lesion in that portion of the brain does.  Near as anyone can tell, lesions in F5 (portion of the inferior frontal gyrus where the mirror neurons on) does not impair action-understanding.  
> >
> >The next, bigger problem for theories of human behavior is that there is no solid evidence of mirror neurons in humans.  A bunch of fmri studies showed a bit of activity in one region, and then meta-studies suggested not that region, maybe some other region,etc.  fmri studies are tricky Google dead salmon fmri. 
> >
> >But even if mirror neurons are involved in humans, there is really strong evidence they can’t be involved in action-understanding.  The mirror proponents suggest speech is a strong trigger for suggested mirror neurons.  For instance, in the speech system, we’ve known since Paul Broca (really old French guy) that lesions can destroy your ability to speak without understanding your ability to understand speech.  This is a huge problem for models that link action-understanding to action, killing those neurons should destroy both.  
> >
> >Also,suggested human mirror neurons do not fire in regards to pantomime actions.  Also in autism spectrum disorders, action-understanding is often impaired with no impairment to action.  
> >
> >So in summary, the simulation theory of empathy got a big resurgence after mirror neurons, but there is decently strong empirical evidence against a mirror-only theory of action-understanding in humans.  That doesn’t mean mirror neurons have no role to play (though if they aren’t found in humans, it does mean they have no role to play), it just means that the brain is complicated.  I think the statement you quoted to me would have been something you could read from a philosopher of mind in the late 80s or early 90s, but not something anyone involved in experiments would say.  By the mid 2000s, a lot of that enthusiasm had pittered a bit.  Then I left the field.  

# Low model percentages

>When you trade algorithmically, a few things stop you from acting on low model percentages
>
>1. you compute an uncertainty on your probability, and your trades need to be risk averse. So it’s not 10^-135, it’s between 0 and 10^-135 or something. Start throwing in huge potential gains to get positive EV, and you’ll see the uncertainties are huge. 
>
>2. In reality, that uncertainty is several percent, nowhere near 10^-130 or even 10^-6. So you really can’t distinguish between 0 and 10^-100 or whatever. 
>
>3. You generally don’t go after the long shots of large gains, you go after the high probability of making little amounts.  
>
>You try to keep an estimate of “model is shit” going by keeping outside validation data and comparing it’s estimates with reality.  
>
>The big fear in all this analysis is big shifts in underlying trends without warning. If the past correlations stop correlating and don’t give you any warning, you can be fucked pretty easily.  
>
>It’s also why you rebuild the models constantly. 

# Cox theorem vs Dempster-Shafer

>There is an idea that lies at the core of the Less Wrong canon that I find a bit pernicious- its an innocent enough little theorem, originally by Cox.  It says- ‘lets say I want to formalize my idea of 'plausibility’, I’m going to create mathematical plausibility scores that have to have certain properties:
>
>1. Often called divisibility/comparability- a plausibility is a real number and it depends on evidence/information. 
>
>2. Often called “common sense”- plausabilities compose in certain logical ways.  If I increase the plausibility of a statement, I decrease the plausibility of NOT that statement,etc
>
>3. Consistency- if I can calculate a plausibility in multiple ways, those ways should agree. 
>
>From these axioms, Cox does a little mathematical exercise to show that the only system that obeys these axioms is standard mathematical probability.  
>
>A HA! Says the LessWrongian- I now have a mathematical framework for reasoning about belief.  Belief/plausability is equivalent to mathematical probability! This means I can use all the theorem of mathematical probability (including the Less Wrong favorite, Bayes)!  
>
>And this really is cool- Cox theorem seems like magic.  Cox outlined everything he wanted in a theorem, made minimal assumptions, and out popped probability!  This is almost too good to be true! 
>
>Well, unfortunately, it IS too good to be true.  There are a lot of non-obvious assumptions glossed over in the first two requirements.  Why SHOULD my belief about a system be reducible to a single number?  Lets say I’m unsure if a coin is fair- so I have a strong belief the coin will produce heads with 30% probability and a weak belief that the coin will produce heads with 50% probability.  Cox says I should represent my belief as a probability, and then combine probabilities to get a single number about the probability of getting heads.  
>
>But I don’t have to do that- I can treat my beliefs separately than the frequency of returning heads (which is obviously a probability, by definition of frequency).  If I treat them separately, I’ll get whats called Dempster-Shafer theory.  Dempster-Shafer is often thought of as a generalization of Bayesian probability, but whats important is that because we have BOTH degrees of belief and probabilities, we side-step 1. in Cox theorem.  Degrees of belief don’t have to behave like probabilities (and often don’t). 
>
>“Alright” I hear you saying (my aren’t you skeptical!) “I CAN do this, but why would I want to do this?  Isn’t this just a stupid artificial counter example?” Not at all- there is a very practical problem where Dempster-Shafer is the go-to solution.  If I have multiple sensors all looking at the same phenomena and I want to combine signals to get a clearer picture, its well known that using standard probabilities and updating with Bayes theorem is often too brittle- conflicting evidence from different sensors can push your probabilities too much.  Dempster-Shafer saves the day. 
>
>Note: there is also something called possibility theory that uses two numbers, possibility and necessity.  I know nothing about it other than it exists and Lofti Zadeh, the fuzzy set guy, invented it.  It can also side step requirement 1. 
>
>“Alright” you say, “but at least I know for situations where number CAN capture my belief, probability is the only way to go.”  Sorry my soon-to-be-disillusioned friend.  Cox packed a lot of assumptions into requirement 2.  In some variants of the proof (see Cox’s original paper for instance) the function that inverts a plausibility, and the functions that compose probabilities have to be twice differentiable, with continuous second derivative.  In more modern variants of the proof, these assumptions can be done away with in favor of assumptions that the set of elements you are assigning plausibilities to are dense.  
>
>This last is actually a big assumption- if the domain of the problem is finite, which is true in many cases, the proof of Cox theorem fails.  See Halpern for a constructive proof of a counter example.  Why does this matter? Because for most arbitrary problems, the set of hypothesis or events (or whatever you want to give plausibility scores to) is finite, especially for toy problems.  
>
>So what does this mean? If we decide to create plausibilities, its a big leap to decide all our information can be captured with a single real number. Even if we make a huge logical leap and work with a single number, Cox theorem is of limited applicability to finite domains.  We can CHOOSE to represent our beliefs as probabilities, but we were ALWAYS able to do that.  The power of Cox was that we HAD to, uniquely, choose probabilities.  This isn’t true.  

# Selected HPMOR posts

These are in danluu's archives, but I reproduce them here anyway:

>It turns out Hermione is winning, so the only way for Draco and Hariezer to try to catch up is to temporarily team up, which leads to a long explanation where Hariezer explains the prisoner’s dilemma and Yudkowsky’s pet decision theory. 
>
>Here is the big problem- In the classic prisoner’s dilemma: 
>
>If my partner cooperates, I can either: 
>
>-cooperate, in which case I spend a short time in jail, and my partner spends a short time in jail
>
>-defect, in which case I spend no time in jail, and my partner serves a long time in jail
>
>If my partner defects, I can either:
>
>-cooperate, in which case I spend a long time in jail, and my partner goes free
>
>-defect, in which case I spend a long time in jail, as does my partner. 
>
>The key insight of the prisoner’s dilemma is that no matter what my partner does, defecting improves my situation. This leads to a dominant strategy where everyone defects, even though the both-defect is worse than the both-cooperate.  
>
>In the situation between Draco and Hariezer:
>
>If Draco cooperates, Hariezer can either:
>
>-cooperate in which case both Hariezer and Draco both have a shot at getting first or second 
>
>-defect, in which case Hariezer is guaranteed second, Draco guaranteed 3rd place
>
>If Draco defects, Hariezer can either 
>
>-cooperate, in which case Hariezer is guaranteed 3rd, and Draco gets 2nd. 
>
>-defect, in which case Hariezer and Draco are fighting it out for 2nd and third.  
>
>Can you see the difference here?  If Draco is expected to cooperate, Hariezer has no incentive to defect- both cooperate is STRICTLY BETTER than the situation where Hariezer defects against Draco.  This is not at all a prisoner’s dilemma, its just cooperating against a bigger threat.  All the pontificating about decision theories that Hariezer does is just wasted breath, because no one is in a prisoner’s dilemma. 
>
>In the second half, Hariezer partners with Draco to get to the bottom of wizarding blood purity.
>
> >Harry Potter had asked how Draco would go about disproving the blood purist hypothesis that wizards couldn’t do the neat stuff now that they’d done eight centuries ago because they had interbred with Muggleborns and Squibs.
>
>Here is the thing about science, step 0 needs to be make sure you’re trying to explain a real phenomena. Hariezer knows this, he tells the story of N-rays earlier in the chapter, but completely fails to understand the point.
>
>Hariezer and Draco have decided, based on one anecdote (the founders of Hogwarts were the best wizards ever, supposedly) that wizards are weaker today than in the past. The first thing they should do is find out if wizards are actually getting weaker. After all, the two most dangerous dark wizards ever were both recent, Grindelwald and Voldemort. Dumbledore is no slouch. Even four students were able to make the marauders map just one generation before Harry. (Incidentally, this is exactly where neoreactionaries often go wrong- they assume things are getting worse without actually checking, and then create elaborate explanations for non-existent facts).
>
>Hariezer’s thinking and plotting are a lot like Sherlock Holmes’s deductions- he consistently takes actions that would result in nothing but tears in reality, but the magic of the plot bends the world around him to conform to his plan. In reality you can’t deduce someone’s wife left him by the fact that his tie doesn’t match his shoes. 
>
>This chapter opens with Hariezer ruminating about how much taking that beating sure has changed his life. He knows how to lose now, he isn’t going to become dark lord now! Quirrell quickly takes him down a peg:
>
> >“Mr. Potter,” he said solemnly, with only a slight grin, “a word of advice. There is such a thing as a performance which is too perfect. Real people who have just been beaten and humiliated for fifteen minutes do not stand up and graciously forgive their enemies. It is the sort of thing you do when you’re trying to convince everyone you’re not Dark, not -“
>
>Hariezer protests, and we get 
>
> >There is nothing you can do to convince me because I would know that was exactly what you were trying to do. And if we are to be even more precise, then while I suppose it is barely possible that perfectly good people exist even though I have never met one, it is nonetheless improbable that someone would be beaten for fifteen minutes and then stand up and feel a great surge of kindly forgiveness for his attackers. On the other hand it is less improbable that a young child would imagine this as the role to play in order to convince his teacher and classmates that he is not the next Dark Lord. The import of an act lies not in what that act resembles on the surface, Mr. Potter, but in the states of mind which make that act more or less probable
>
>How does Hariezer take this?  Does he point out “if no evidence can sway your priors, your priors are too strong?” or some other bit of logic-chop Bayes-judo?  Nope, he drops some nonsensical jargon: 
>
> >Harry blinked. He’d just had the dichotomy between the representativeness heuristic and the Bayesian definition of evidence explained to him by a wizard.
>
>Where is Quirrell using bayesian evidence? He isn’t, he is neglecting all evidence because all evidence fits his hypothesis. Where does the representativeness heuristic come into play? It doesn’t.  
>
>The representative heuristic is making estimates based on how typical of a class something is.  i.e. show someone a picture of a stereotypical ‘nerd’ and say “is this person more likely an english or a physics grad student?” The representative heuristic says “you should answer physics.”  Its a good rule-of-thumb that psychologists think is probably hardwired into us. It also leads to some well-known fallacies I won’t get into here.  
>
>Quirrell is of course doing none of that- Quirrell has a hypothesis that fits anything Hariezer could do, so no amount of evidence will dissuade him. 
>
> Hariezer drops this on us, regarding the brain’s ability to understand time travel: 
> 
> >Now, for the first time, he was up against the prospect of a mystery that was threatening to be permanent…. it was entirely possible that his human mind never could understand, because his brain was made of old-fashioned linear-time neurons, and this had turned out to be an impoverished subset of reality.
> 
> This seems to misunderstand the nature of mathematics and its relation to science.  I can’t visualize a 4 dimensional curved space,certainly not the way I visualize 2 and 3d objects.  But that doesn’t stop me from describing it and working with it as a mathematical object.  
> 
> Time is ALREADY very strange and impossible to visualize.  But mathematics allows us to go beyond what our brain can visualize to create notations and languages that let us deal with anything we can formalize and that has consistent rules.  Its amazingly powerful.  
> 
> I never thought I’d see Hariezer Yudotter, who just a few chapters back was claiming science could let us perfectly manipulate and control people (better than an imperio curse, or whatever the spell that lets you control people) argue that science/mathematics couldn’t deal with linear time.  

## Humor

>Problem: Justify the use of induction in science
>
>Answer: Well, it’s worked so far. 
>
>AwesomePants McAwesomeSauce was the most awesome person who had ever lived.  He had once played music so awesome that every woman listening had immediately become with child.  Those children would grow up to become Jesus, Mohammed, Buddha and The Beatles.  
>
>He once fought off an army of memons (memons are the awesome scientific version of demons.  Only fools and peasants believe in demons) with nothing but his awesome left fist.  His right fist is saved for real emergencies.  
>
>But you see, AwesomePants has a problem, having hit level 99 he has no one left to teach him.  So AwesomePants is concentrating all his awesome into a single ball of awesome using his mastery of phagic (which is like magic, only its real).  This will obviously create an awesome black hole which will draw from the future the only teacher who can possibly help him- the future version of AwesomePants himself. 
>
>I walk into the offices of a small startup to pitch my services, wearing a suit because I had just come from a meeting with a bank. 
>
>Startup guy (SG): “Hey, glad you came in…. So… ”
>
>Me: “Oh, the suit? I work for a wide range of clients, from banks to startups, so I find its best to wear standard attire" 
>
><Forty five minutes go by, in which I pitch myself>
>
>SG: "So, you are make a good case… but…. ”
>
>Me: “Jesus Christ, is this about the suit still?" 
>
>SG: "Its just that we value a certain degree of creative non-conformity.”
>
>Me: “Well, then you need to hire me, I’m obviously non-conformist, I’m the only person here wearing a suit!" 
>
>Needless to say, I never heard from them. 
>
>“Obviously, in the open spaces of the Savanna, a car was and is far more useful for hunting than a jet plane, and evolutionary selection would have favored drivers to jet flyers.  
>
>And, indeed, when we tested a few hundred people we discovered that even today the average person is much more competent at driving a car than flying a jet. This is strong support for the evolutionary paradigm, and the driving-a-car brain module.” 
>
>There is a 10^(-80) probability I can open a portal to dimension-X-Earth in my basement, if you give me $80,000 a year for the next decade. We can colonize that extra-dimensional Earth, which will safeguard humanity in the event Earth gets destroyed, so extrapolating into the future thats at least pi*3^^^^^^^^^^3 utilons, so I’m the most effective charity you can donate to ever.  
>
> >Death Star type ray weapons are obviously not an x-risk; by the time they’re a consideration,
>
>I’m actually working very hard on one in my basement, and I predict they’ll be created by accident by experiments like the LHC within 10-100 years. Luckily, I’ve started a think tank to make sure that the super colliders of the future will be provably safe. I just need money to fund my research efforts. 
>
>My girlfriend: “What have you been working on over there?” 
>
>Me: “Uhhhh… so…. there is this horrible Harry Potter fan fiction… you know, when people on the internet write more stories about Harry Potter? Yea, that. Anyway, this one is pretty terrible so I thought I’d read it and complain about it on the internet…. So I’m listening to me say this out loud and it sounds ridiculous, but.. well, it IS ridiculous… but…” 
>
>I thought I was out of the game.  My laptop was gathering dust, my black belt in frequentist kickboxing framed on the wall.  Even the dreams of being boxed by the AI had stopped.  I was saving up for the white house, the picket fence, the 2.5 children.  
>
>And then she came through the door, a regular June Cleaver.  Her son was showing all the signs.  It had started slow, he’d gone off the carbs, said that Aumann’s agreement theorem demonstrated the superiority of anime over other forms of entertainment.  Next thing they knew, grandparents are frozen in the garage.  He was practically snorting many worlds quantum, and was probably one step away from freebayesing.
>
>I unholstered my compressed air,dusted and opened my laptop.  One last job, one last soul to save.  Then retirement… 
